{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypOsVoLPqZIc"
   },
   "source": [
    "# CV05-1 Mask R-CNN\n",
    "\n",
    "### Lab Description\n",
    "This lab introduces **Mask R-CNN**, a deep learning model for **instance segmentation**.  \n",
    "Unlike image classification or object detection, instance segmentation predicts **both bounding boxes and pixel-wise masks** for each object.\n",
    "\n",
    "You will:\n",
    "- Fine-tune a **Mask R-CNN (ResNet-50 FPN backbone)** on a small COCO dataset subset.\n",
    "- Monitor **epoch training time, batch size, VRAM usage**.\n",
    "- Evaluate the trained model and **visualize masks & bounding boxes**.\n",
    "\n",
    "### What you can expect to learn\n",
    "- Understand the difference between **object detection** and **instance segmentation**.  \n",
    "- Use **torchvision’s Mask R-CNN** for fine-tuning.  \n",
    "- Record and analyze **training performance metrics**.  \n",
    "- Visualize predicted **object masks and bounding boxes**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9480,
     "status": "ok",
     "timestamp": 1757762718557,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "-d4ReXMCqNTa"
   },
   "outputs": [],
   "source": [
    "# ⚙️ Setup & Config\n",
    "!pip install torch torchvision --quiet\n",
    "!pip install pycocotools matplotlib pandas --quiet\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1757762718562,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "LT-q0DAaqdeK"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    batch_size: int = 2\n",
    "    epochs: int = 3\n",
    "    lr: float = 1e-4\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "\n",
    "# Seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Output dir\n",
    "OUTPUT_DIR = os.path.expanduser(\"./output_maskrcnn\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCnuPrK3qhWC"
   },
   "source": [
    "## 2. Dataset: COCO128 (mini COCO)\n",
    "We use `COCO128`, a lightweight subset of COCO2017 with bounding boxes and segmentation masks.  \n",
    "This allows us to train Mask R-CNN quickly inside Colab.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1125,
     "status": "ok",
     "timestamp": 1757763151007,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "Ns8RH01is9oJ",
    "outputId": "9d61a4ce-f449-45ee-e33c-7658975483c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 6819k  100 6819k    0     0  9565k      0 --:--:-- --:--:-- --:--:-- 9565k\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./data/coco128\n",
    "!rm -f coco128.zip\n",
    "!curl -L \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip\" -o coco128.zip\n",
    "!unzip -q coco128.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "executionInfo": {
     "elapsed": 55160,
     "status": "error",
     "timestamp": 1757763091213,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "q9BaF1BvqeG8",
    "outputId": "07a19e1d-4aba-4890-d6d7-5ff21d74a9ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 6819k  100 6819k    0     0   9.8M      0 --:--:-- --:--:-- --:--:--  9.8M\n",
      "replace ./data/coco128/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: all\n",
      "error:  invalid response [all]\n",
      "replace ./data/coco128/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000612.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000404.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000438.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000389.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000564.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000149.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000605.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000349.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000201.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000599.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "replace ./data/coco128/images/train2017/000000000572.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: loading annotations into memory...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1662315017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# ---- Build dataset & loader ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m train_dataset = COCOMask(\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mimg_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/coco128/images/train2017\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mannFile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/coco128/annotations/instances_train2017.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1662315017.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_folder, annFile, transform)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCOCOMask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, annFile, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done (t={:0.2f}s)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \"\"\"\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "# ---- Transform ----\n",
    "train_tfms = T.Compose([T.ToTensor()])\n",
    "\n",
    "\n",
    "# ---- Custom dataset for Mask R-CNN (COCO format -> tensors with masks) ----\n",
    "class COCOMask(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, annFile, transform=None):\n",
    "        super().__init__(img_folder, annFile)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, anns = super().__getitem__(idx)  # img: PIL.Image, anns: list of dicts\n",
    "        img = img.convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes, labels, masks, iscrowd, areas = [], [], [], [], []\n",
    "\n",
    "        for obj in anns:\n",
    "            # 1) boxes (xywh -> xyxy)\n",
    "            x, y, bw, bh = obj[\"bbox\"]\n",
    "            if bw <= 0 or bh <= 0:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = x, y, x + bw, y + bh\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "            # 2) label (use COCO category_id; our head = 91 classes)\n",
    "            labels.append(int(obj[\"category_id\"]))\n",
    "\n",
    "            # 3) mask (handle polygon list or RLE)\n",
    "            seg = obj.get(\"segmentation\", None)\n",
    "            m = None\n",
    "            if seg:\n",
    "                if isinstance(seg, list):\n",
    "                    # polygon -> RLE -> decode\n",
    "                    rles = coco_mask.frPyObjects(seg, h, w)\n",
    "                    rle = coco_mask.merge(rles)\n",
    "                    m = coco_mask.decode(rle)\n",
    "                elif isinstance(seg, dict):  # already RLE\n",
    "                    m = coco_mask.decode(seg)\n",
    "            if m is None:\n",
    "                # fall back to bbox mask if missing seg (rare in coco128)\n",
    "                m = np.zeros((h, w), dtype=np.uint8)\n",
    "                m[int(y1) : int(y2), int(x1) : int(x2)] = 1\n",
    "            if m.ndim == 3:  # merge multiple RLEs\n",
    "                m = np.any(m, axis=2).astype(np.uint8)\n",
    "            masks.append(torch.as_tensor(m, dtype=torch.uint8))\n",
    "\n",
    "            # 4) area / iscrowd\n",
    "            areas.append(float(obj.get(\"area\", bw * bh)))\n",
    "            iscrowd.append(int(obj.get(\"iscrowd\", 0)))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            masks = torch.zeros((0, h, w), dtype=torch.uint8)\n",
    "            iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.stack(masks, dim=0)  # [N, H, W]\n",
    "            iscrowd = torch.tensor(iscrowd, dtype=torch.int64)\n",
    "            areas = torch.tensor(areas, dtype=torch.float32)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": areas,\n",
    "            \"iscrowd\": iscrowd,\n",
    "        }\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = T.ToTensor()(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "# ---- Build dataset & loader ----\n",
    "train_dataset = COCOMask(\n",
    "    img_folder=\"./data/coco128/images/train2017\",\n",
    "    annFile=\"./data/coco128/annotations/instances_train2017.json\",\n",
    "    transform=train_tfms,\n",
    ")\n",
    "\n",
    "\n",
    "# Important: custom collate_fn to keep lists (Mask R-CNN expects list[Tensor]/list[Dict])\n",
    "def collate_fn(batch):\n",
    "    imgs, targets = list(zip(*batch))\n",
    "    return list(imgs), list(targets)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7ESpAlOqoCO"
   },
   "source": [
    "## 3. Model\n",
    "We use **Mask R-CNN with ResNet-50 FPN backbone**, pretrained on COCO.  \n",
    "We replace the **classification head** and **mask head** to match COCO’s 91 classes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "aborted",
     "timestamp": 1757760539622,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "-Gy3-yGKqpjm"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# Replace heads for fine-tuning\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 91)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, 256, 91)\n",
    "\n",
    "model = model.to(cfg.device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb2VwwiRqrL4"
   },
   "source": [
    "## 4. Training\n",
    "We train for a few epochs and log:\n",
    "- **Loss**\n",
    "- **Time per epoch**\n",
    "- **Peak VRAM usage**\n",
    "- **Batch size**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1757760539626,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "0xGbmqekqtnw"
   },
   "outputs": [],
   "source": [
    "def get_vram_mb():\n",
    "    return torch.cuda.max_memory_allocated(cfg.device) / (1024**2) if torch.cuda.is_available() else 0\n",
    "\n",
    "\n",
    "history = {\"epoch\": [], \"loss\": [], \"time\": [], \"vram_MB\": [], \"batch_size\": []}\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for imgs, targets in train_loader:\n",
    "        imgs = [img.to(cfg.device) for img in imgs]\n",
    "        targets = [{k: v.to(cfg.device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(imgs, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    t1 = time.time()\n",
    "    epoch_time = t1 - t0\n",
    "    vram = get_vram_mb()\n",
    "\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"loss\"].append(total_loss / len(train_loader))\n",
    "    history[\"time\"].append(epoch_time)\n",
    "    history[\"vram_MB\"].append(vram)\n",
    "    history[\"batch_size\"].append(cfg.batch_size)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{cfg.epochs} | Loss {total_loss / len(train_loader):.4f} | \"\n",
    "        f\"Time {epoch_time:.1f}s | VRAM {vram:.1f}MB\"\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(history)\n",
    "df.to_csv(os.path.join(OUTPUT_DIR, \"maskrcnn_log.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSYfc90kqweY"
   },
   "source": [
    "## 5. Evaluation & Visualization\n",
    "We run inference on a few images and overlay:\n",
    "- **Predicted masks**\n",
    "- **Bounding boxes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1757760539630,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "ea6fI2gUqxnr"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_imgs, _ = next(iter(train_loader))\n",
    "test_imgs = [img.to(cfg.device) for img in test_imgs]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_imgs)\n",
    "\n",
    "\n",
    "# Visualization\n",
    "def show_masks(img, masks, boxes, scores, threshold=0.5):\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    for m, box, score in zip(masks, boxes, scores):\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        mask = m[0].cpu().numpy()\n",
    "        plt.imshow(mask, alpha=0.4)\n",
    "        x1, y1, x2, y2 = box.cpu().numpy()\n",
    "        plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color=\"red\", linewidth=2))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(min(2, len(test_imgs))):\n",
    "    show_masks(test_imgs[i], outputs[i][\"masks\"], outputs[i][\"boxes\"], outputs[i][\"scores\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMU04wn+bq+Wo9hqRVSJFc6",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
