{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235804d6-e158-45fd-a6ae-283a623f38d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.19\n",
      "PyTorch: 2.10.0.dev20251101+rocm7.0\n",
      "Torchvision: 0.25.0.dev20251102+rocm7.0\n",
      "CUDA available: True\n",
      "CUDA device: AMD Radeon AI PRO R9700\n"
     ]
    }
   ],
   "source": [
    "import sys, platform, torch, torchvision\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18685e1e-fdbe-4847-8c55-cb36b4918965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f180eaf3-8269-4388-acd4-a29bf21571c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "config = {\n",
    "    \"dataset\": {\n",
    "        \"root\": \"~/data/flowers102\",   \n",
    "        \"image_size\": 64,              \n",
    "    },\n",
    "    \"diffusion\": {\n",
    "        \"num_timesteps\": 1000,\n",
    "        \"beta_start\": 1e-4,\n",
    "        \"beta_end\": 2e-2,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"im_channels\": 3,              # RGB\n",
    "        \"im_size\": 64,\n",
    "        \"down_channels\": [128, 256, 512, 512],\n",
    "        \"mid_channels\": [512, 512, 512],\n",
    "        \"down_sample\": [True, True, False],\n",
    "        \"time_emb_dim\": 256,\n",
    "        \"num_down_layers\": 3,\n",
    "        \"num_mid_layers\": 3,\n",
    "        \"num_up_layers\": 3,\n",
    "        \"num_heads\": 4,\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"task_name\": \"flowers_ddpm\",\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 2000,\n",
    "        \"lr\": 1e-4,\n",
    "        \"ckpt_name\": \"ddpm_flowers102.ckpt\",\n",
    "        \"num_samples\": 4,             \n",
    "        \"num_grid_rows\": 2,            \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "Path(config[\"train\"][\"task_name\"]).mkdir(parents=True, exist_ok=True)\n",
    "(Path(config[\"train\"][\"task_name\"]) / \"samples\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea05d6dd-58dc-4083-a86f-82bc076d9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseScheduler:\n",
    "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        # linear beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1.0 - self.alpha_cum_prod)\n",
    "\n",
    "    def add_noise(self, x0, noise, t):\n",
    "        \"\"\"\n",
    "        forward process: q(x_t | x_0)\n",
    "        x_t = sqrt(alpha_bar_t) * x0 + sqrt(1 - alpha_bar_t) * noise\n",
    "\n",
    "        x0: (B, C, H, W)\n",
    "        noise: (B, C, H, W)\n",
    "        t: (B,)  each sample's timestep\n",
    "        \"\"\"\n",
    "        B = x0.shape[0]\n",
    "\n",
    "        sqrt_alpha_cum = self.sqrt_alpha_cum_prod.to(x0.device)[t].reshape(B)\n",
    "        sqrt_one_minus = self.sqrt_one_minus_alpha_cum_prod.to(x0.device)[t].reshape(B)\n",
    "\n",
    "        # reshape \n",
    "        while len(sqrt_alpha_cum.shape) < len(x0.shape):\n",
    "            sqrt_alpha_cum = sqrt_alpha_cum.unsqueeze(-1)\n",
    "            sqrt_one_minus = sqrt_one_minus.unsqueeze(-1)\n",
    "\n",
    "        return sqrt_alpha_cum * x0 + sqrt_one_minus * noise\n",
    "\n",
    "    def sample_prev_timestep(self, xt, noise_pred, t_scalar):\n",
    "        \"\"\"\n",
    "        reverse process: p_theta(x_{t-1} | x_t)\n",
    "\n",
    "        xt: (B, C, H, W)\n",
    "        noise_pred: (B, C, H, W)\n",
    "        t_scalar: use same t for each batch\n",
    "        \"\"\"\n",
    "        if isinstance(t_scalar, torch.Tensor):\n",
    "            t_int = int(t_scalar.item())\n",
    "        else:\n",
    "            t_int = int(t_scalar)\n",
    "\n",
    "        betas = self.betas.to(xt.device)\n",
    "        alphas = self.alphas.to(xt.device)\n",
    "        alpha_cum = self.alpha_cum_prod.to(xt.device)\n",
    "        sqrt_one_minus = self.sqrt_one_minus_alpha_cum_prod.to(xt.device)\n",
    "\n",
    "        beta_t = betas[t_int]\n",
    "        alpha_t = alphas[t_int]\n",
    "        alpha_bar_t = alpha_cum[t_int]\n",
    "        sqrt_one_minus_t = sqrt_one_minus[t_int]\n",
    "\n",
    "        x0_pred = (xt - sqrt_one_minus_t * noise_pred) / torch.sqrt(alpha_bar_t)\n",
    "        x0_pred = torch.clamp(x0_pred, -1.0, 1.0)\n",
    "\n",
    "        mean = (xt - beta_t * noise_pred / sqrt_one_minus_t) / torch.sqrt(alpha_t)\n",
    "\n",
    "        if t_int == 0:\n",
    "            return mean, x0_pred\n",
    "        else:\n",
    "            alpha_bar_prev = alpha_cum[t_int - 1]\n",
    "            var = (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t) * beta_t\n",
    "            sigma = torch.sqrt(var)\n",
    "            z = torch.randn_like(xt)\n",
    "            return mean + sigma * z, x0_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e030667-fcc0-46c7-870f-71918bcd0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    \"\"\"\n",
    "    sinusoidal time embedding, like Transformer positional encoding\n",
    "    time_steps: (B,)\n",
    "    return: (B, temb_dim)\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "\n",
    "    half_dim = temb_dim // 2\n",
    "    exponent = torch.arange(half_dim, dtype=torch.float32, device=time_steps.device) / half_dim\n",
    "    factor = 10000 ** exponent  \n",
    "\n",
    "    # time steps reshape: (B, 1) -> (B, half_dim)\n",
    "    t = time_steps.float().unsqueeze(1)\n",
    "    t = t / factor.unsqueeze(0)\n",
    "\n",
    "    emb = torch.cat([torch.sin(t), torch.cos(t)], dim=-1)\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bca30e7-e6e1-46a9-9090-78fa2efc87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample=True, num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "\n",
    "        self.resnet_conv_first = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels,\n",
    "                          out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attention_norms = nn.ModuleList([\n",
    "            nn.GroupNorm(8, out_channels)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels if i == 0 else out_channels,\n",
    "                      out_channels, kernel_size=1)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.down_sample_conv = (\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "            if self.down_sample else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # ResNet block\n",
    "            res_in = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](res_in)\n",
    "\n",
    "            # Self-attention\n",
    "            B, C, H, W = out.shape\n",
    "            attn_in = out.view(B, C, H * W)\n",
    "            attn_in = self.attention_norms[i](attn_in)\n",
    "            attn_in = attn_in.transpose(1, 2)   # (B, HW, C)\n",
    "            attn_out, _ = self.attentions[i](attn_in, attn_in, attn_in)\n",
    "            attn_out = attn_out.transpose(1, 2).view(B, C, H, W)\n",
    "            out = out + attn_out\n",
    "\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03879fd-6646-4cdc-a024-6f812fe6a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 num_heads=4, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.resnet_conv_first = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels,\n",
    "                          out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "            for i in range(num_layers + 1)\n",
    "        ])\n",
    "\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers + 1)\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "            for _ in range(num_layers + 1)\n",
    "        ])\n",
    "\n",
    "        self.attention_norms = nn.ModuleList([\n",
    "            nn.GroupNorm(8, out_channels)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels if i == 0 else out_channels,\n",
    "                      out_channels, kernel_size=1)\n",
    "            for i in range(num_layers + 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "\n",
    "        res_in = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](res_in)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            B, C, H, W = out.shape\n",
    "            attn_in = out.view(B, C, H * W)\n",
    "            attn_in = self.attention_norms[i](attn_in)\n",
    "            attn_in = attn_in.transpose(1, 2)\n",
    "            attn_out, _ = self.attentions[i](attn_in, attn_in, attn_in)\n",
    "            attn_out = attn_out.transpose(1, 2).view(B, C, H, W)\n",
    "            out = out + attn_out\n",
    "\n",
    "            res_in = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](res_in)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54a60a-6287-4da2-8c6c-b8c430fc006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_main_channels,   \n",
    "        skip_channels,      \n",
    "        out_channels,       \n",
    "        t_emb_dim,\n",
    "        up_sample=True,\n",
    "        num_heads=4,\n",
    "        num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "\n",
    "        self.up_sample_conv = (\n",
    "            nn.ConvTranspose2d(\n",
    "                in_main_channels, in_main_channels,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            )\n",
    "            if self.up_sample else nn.Identity()\n",
    "        )\n",
    "\n",
    "        in_after_concat = in_main_channels + skip_channels\n",
    "\n",
    "        self.resnet_conv_first = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, in_after_concat if i == 0 else out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(\n",
    "                    in_after_concat if i == 0 else out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3, stride=1, padding=1\n",
    "                ),\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.resnet_conv_second = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8, out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3, stride=1, padding=1\n",
    "                ),\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attention_norms = nn.ModuleList([\n",
    "            nn.GroupNorm(8, out_channels)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_after_concat if i == 0 else out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        \"\"\"\n",
    "        x: up block 的 feature (B, C_main, H, W)\n",
    "        out_down: skip connection 的 feature (B, C_skip, H or 2H, W or 2W)\n",
    "        \"\"\"\n",
    "        x = self.up_sample_conv(x)          \n",
    "        x = torch.cat([x, out_down], dim=1) #  concat skip: (B, C_main + C_skip, H, W)\n",
    "\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            res_in = out\n",
    "            # ResNet conv1\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            # ResNet conv2\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            \n",
    "            out = out + self.residual_input_conv[i](res_in)\n",
    "\n",
    "            # Self-attention\n",
    "            B, C, H, W = out.shape\n",
    "            attn_in = out.view(B, C, H * W)\n",
    "            attn_in = self.attention_norms[i](attn_in)\n",
    "            attn_in = attn_in.transpose(1, 2)         # (B, HW, C)\n",
    "            attn_out, _ = self.attentions[i](attn_in, attn_in, attn_in)\n",
    "            attn_out = attn_out.transpose(1, 2).view(B, C, H, W)\n",
    "            out = out + attn_out\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4edca7d7-ded3-4e25-8474-52c71327ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "\n",
    "        im_channels = model_config[\"im_channels\"]\n",
    "        self.down_channels = model_config[\"down_channels\"]\n",
    "        self.mid_channels = model_config[\"mid_channels\"]\n",
    "        self.down_sample = model_config[\"down_sample\"]\n",
    "        self.t_emb_dim = model_config[\"time_emb_dim\"]\n",
    "        self.num_down_layers = model_config[\"num_down_layers\"]\n",
    "        self.num_mid_layers = model_config[\"num_mid_layers\"]\n",
    "        self.num_up_layers = model_config[\"num_up_layers\"]\n",
    "        num_heads = model_config[\"num_heads\"]\n",
    "\n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-1]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "\n",
    "        # time embedding MLP\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "        )\n",
    "\n",
    "        # input conv\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n",
    "\n",
    "        # Down blocks\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(\n",
    "                DownBlock(\n",
    "                    in_channels=self.down_channels[i],\n",
    "                    out_channels=self.down_channels[i + 1],\n",
    "                    t_emb_dim=self.t_emb_dim,\n",
    "                    down_sample=self.down_sample[i],\n",
    "                    num_heads=num_heads,\n",
    "                    num_layers=self.num_down_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Mid blocks\n",
    "        self.mids = nn.ModuleList()\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(\n",
    "                MidBlock(\n",
    "                    in_channels=self.mid_channels[i],\n",
    "                    out_channels=self.mid_channels[i + 1],\n",
    "                    t_emb_dim=self.t_emb_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    num_layers=self.num_mid_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        current_channels = self.mid_channels[-1]  \n",
    "\n",
    "        for i in reversed(range(len(self.down_channels) - 1)):\n",
    "            skip_ch = self.down_channels[i]     \n",
    "\n",
    "            out_ch = self.down_channels[i] if i > 0 else 32\n",
    "\n",
    "            self.ups.append(\n",
    "                UpBlock(\n",
    "                    in_main_channels=current_channels,\n",
    "                    skip_channels=skip_ch,\n",
    "                    out_channels=out_ch,\n",
    "                    t_emb_dim=self.t_emb_dim,\n",
    "                    up_sample=self.down_sample[i],\n",
    "                    num_heads=num_heads,\n",
    "                    num_layers=self.num_up_layers,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            current_channels = out_ch \n",
    "\n",
    "        self.final_channels = current_channels\n",
    "        self.norm_out = nn.GroupNorm(8, self.final_channels)\n",
    "        self.conv_out = nn.Conv2d(self.final_channels, im_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x: (B, C, H, W)\n",
    "        # t: (B,)  timestep for each image\n",
    "        B = x.shape[0]\n",
    "\n",
    "        out = self.conv_in(x)  # -> (B, C1, H, W)\n",
    "\n",
    "        t = t.view(B)  # -> (B,)\n",
    "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "\n",
    "        # Down path\n",
    "        down_outs = []\n",
    "        for down in self.downs:\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "\n",
    "        # Mid blocks\n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "\n",
    "        # Up path\n",
    "        for up in self.ups:\n",
    "            skip = down_outs.pop()\n",
    "            out = up(out, skip, t_emb)\n",
    "\n",
    "        out = self.norm_out(out)\n",
    "        out = F.silu(out)\n",
    "        out = self.conv_out(out)  #input shape = output shape (B, C, H, W)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e972e-c4b9-40fa-96b6-2a4b30c0ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def debug_one_step_denoise(model, scheduler, train_loader, t_value=500, n_samples=6):\n",
    "    model.eval()\n",
    "\n",
    "    imgs, _ = next(iter(train_loader))  # 1 batch\n",
    "    imgs = imgs[:n_samples].to(device)\n",
    "\n",
    "    B = imgs.size(0)\n",
    "    t = torch.full((B,), t_value, device=device, dtype=torch.long)\n",
    "\n",
    "    # add noise\n",
    "    noise = torch.randn_like(imgs)\n",
    "    x_t = scheduler.add_noise(imgs, noise, t)\n",
    "\n",
    "    # predict noise\n",
    "    noise_pred = model(x_t, t)\n",
    "\n",
    "    alpha_bar_t = scheduler.alpha_cum_prod.to(device)[t_value]\n",
    "    sqrt_one_minus_t = scheduler.sqrt_one_minus_alpha_cum_prod.to(device)[t_value]\n",
    "    x0_pred = (x_t - sqrt_one_minus_t * noise_pred) / torch.sqrt(alpha_bar_t)\n",
    "    x0_pred = torch.clamp(x0_pred, -1.0, 1.0)\n",
    "\n",
    "    to_01 = lambda x: (torch.clamp(x, -1, 1) + 1) / 2\n",
    "\n",
    "    grid_real  = make_grid(to_01(imgs), nrow=n_samples)\n",
    "    grid_noisy = make_grid(to_01(x_t), nrow=n_samples)\n",
    "    grid_pred  = make_grid(to_01(x0_pred), nrow=n_samples)\n",
    "\n",
    "    plt.figure(figsize=(14,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Real x0\")\n",
    "    plt.imshow(grid_real.permute(1,2,0).cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(f\"x_t (t={t_value})\")\n",
    "    plt.imshow(grid_noisy.permute(1,2,0).cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Predicted x0 (denoised)\")\n",
    "    plt.imshow(grid_pred.permute(1,2,0).cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b5f501-4b94-4b50-a23f-c74b3c232234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = config[\"dataset\"][\"image_size\"]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    #  map from [0,1] to [-1,1]： (x - 0.5) / 0.5\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],\n",
    "                         [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "train_dataset = Flowers102(\n",
    "    root=config[\"dataset\"][\"root\"],\n",
    "    split=\"train\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"train\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "len(train_dataset), len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c844a8-d3db-41a6-9ba9-9dac7ce2b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cfg = config[\"diffusion\"]\n",
    "model_cfg = config[\"model\"]\n",
    "train_cfg = config[\"train\"]\n",
    "\n",
    "# build scheduler\n",
    "scheduler = LinearNoiseScheduler(\n",
    "    num_timesteps=diff_cfg[\"num_timesteps\"],\n",
    "    beta_start=diff_cfg[\"beta_start\"],\n",
    "    beta_end=diff_cfg[\"beta_end\"],\n",
    ")\n",
    "\n",
    "# build model\n",
    "model = UNet(model_cfg).to(device)\n",
    "\n",
    "# optimizer / loss\n",
    "optimizer = Adam(model.parameters(), lr=train_cfg[\"lr\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# checkpoint path\n",
    "ckpt_path = Path(train_cfg[\"task_name\"]) / train_cfg[\"ckpt_name\"]\n",
    "\n",
    "# if pretrained\n",
    "if ckpt_path.exists():\n",
    "    print(f\"Loading checkpoint from {ckpt_path}\")\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472da74-bc8c-4530-98c9-42276c5fe75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000:  12%|██████▉                                                | 16/128 [00:36<04:11,  2.25s/it, loss=0.7443]"
     ]
    }
   ],
   "source": [
    "num_epochs = train_cfg[\"num_epochs\"]\n",
    "epoch_losses = []   \n",
    "\n",
    "# log file path\n",
    "log_path = Path(train_cfg[\"task_name\"]) / \"train_log.txt\"\n",
    "log_file = open(log_path, \"a\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for imgs, _ in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "\n",
    "        # 1. sample gaussian noise\n",
    "        noise = torch.randn_like(imgs)\n",
    "\n",
    "        # 2. sample timesteps t ~ Uniform({0,...,T-1})\n",
    "        B = imgs.shape[0]\n",
    "        t = torch.randint(\n",
    "            low=0,\n",
    "            high=diff_cfg[\"num_timesteps\"],\n",
    "            size=(B,),\n",
    "            device=device,\n",
    "        ).long()\n",
    "\n",
    "        # 3. forward process: get x_t\n",
    "        noisy_imgs = scheduler.add_noise(imgs, noise, t)\n",
    "\n",
    "        # 4. Use U-Net to predict noise\n",
    "        noise_pred = model(noisy_imgs, t)\n",
    "\n",
    "        # 5. loss = MSE(noise_pred, noise_real)\n",
    "        loss = criterion(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    epoch_losses.append(avg_loss)  \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - loss: {avg_loss:.4f}\")\n",
    "    # write to log file\n",
    "    log_file.write(f\"Epoch {epoch+1}/{num_epochs} - loss: {avg_loss:.4f}\\n\")\n",
    "    log_file.flush()\n",
    "\n",
    "    # save ckpt each epoch\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "log_file.close()\n",
    "\n",
    "print(\"Training log saved to:\", log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6293803-9552-4e08-81af-558b3dc9d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=99, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38f373-629f-40ac-bb2d-ccc4e53eacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=199, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161868b-f95e-4333-bd6e-7f729071ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=299, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b21d6f5-65d6-4a34-957d-2a7082798a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=399, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf105bc-2c60-40bb-ac06-c61220f2e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=499, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703502f-007c-4ab9-82f8-6ef6a9883c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=599, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5db8e-a4fc-4daf-8770-f4b20634f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=699, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf49e6-2b39-4176-95c1-65793fd77796",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=799, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0499b-b683-44ca-9cd8-ced8f286d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=899, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84698809-cc03-4bf0-a0a1-674ac3e7fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_step_denoise(model, scheduler, train_loader, t_value=999, n_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56276a74-87c0-43b1-a5e1-0cc41a5c4631",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_flowers(model, scheduler, num_samples=16, num_grid_rows=4, save_name=\"sample.png\"):\n",
    "    \n",
    "    model.eval()\n",
    "    T = scheduler.num_timesteps\n",
    "\n",
    "    betas = scheduler.betas.to(device)\n",
    "    alphas = scheduler.alphas.to(device)\n",
    "    alpha_cum = scheduler.alpha_cum_prod.to(device)\n",
    "    sqrt_one_minus_all = scheduler.sqrt_one_minus_alpha_cum_prod.to(device)\n",
    "\n",
    "    # x_T:  Gaussian noise\n",
    "    xt = torch.randn(\n",
    "        num_samples,\n",
    "        config[\"model\"][\"im_channels\"],\n",
    "        config[\"model\"][\"im_size\"],\n",
    "        config[\"model\"][\"im_size\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        t_batch = torch.full(\n",
    "            (xt.shape[0],),\n",
    "            t,\n",
    "            device=device,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "e\n",
    "        noise_pred = model(xt, t_batch)\n",
    "\n",
    "        beta_t = betas[t]\n",
    "        alpha_t = alphas[t]\n",
    "        alpha_bar_t = alpha_cum[t]\n",
    "        sqrt_one_minus_t = sqrt_one_minus_all[t]\n",
    "\n",
    "        x0_pred = (xt - sqrt_one_minus_t * noise_pred) / torch.sqrt(alpha_bar_t)\n",
    "        x0_pred = torch.clamp(x0_pred, -1.0, 1.0)\n",
    "\n",
    "        mean = (xt - beta_t * noise_pred / sqrt_one_minus_t) / torch.sqrt(alpha_t)\n",
    "\n",
    "        if t > 0:\n",
    "            alpha_bar_prev = alpha_cum[t - 1]\n",
    "            var = (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t) * beta_t\n",
    "            sigma = torch.sqrt(var)\n",
    "            z = torch.randn_like(xt)\n",
    "            xt = mean + sigma * z\n",
    "        else:\n",
    "            xt = mean\n",
    "\n",
    "    ims = torch.clamp(xt, -1.0, 1.0).cpu()\n",
    "    ims = (ims + 1.0) / 2.0  # [-1,1] → [0,1]\n",
    "\n",
    "    grid = make_grid(ims, nrow=num_grid_rows)\n",
    "    np_grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(np_grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    out_path = Path(config[\"train\"][\"task_name\"]) / \"samples\" / save_name\n",
    "    torchvision.utils.save_image(grid, out_path)\n",
    "    print(\"Saved samples to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356019c-6490-4b88-b62f-3cc35feefff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model ckpt\n",
    "if ckpt_path.exists():\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    print(\"Loaded trained weights from\", ckpt_path)\n",
    "else:\n",
    "    print(\"Warning: checkpoint not found, sampling from randomly initialized model\")\n",
    "\n",
    "sample_flowers(\n",
    "    model,\n",
    "    scheduler,\n",
    "    num_samples=4,\n",
    "    num_grid_rows=2,\n",
    "    save_name=\"flowers_epoch_last.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15e7e1-e8cb-4d38-83fc-35d3239410e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv_env)",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
