{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55187e",
   "metadata": {
    "id": "1a55187e"
   },
   "source": [
    "# DL12 Transformer\n",
    "\n",
    "### Lab Description\n",
    "\n",
    "This laboratory exercise introduces the **Transformer** architecture by implementing a simplified version from scratch using PyTorch. The Transformer is a powerful sequence model widely used in natural language processing tasks such as machine translation, text generation, and language understanding.\n",
    "\n",
    "In this lab, you will build a mini Transformer to model character-level language generation. The model will learn to predict the next character in a sequence given a small text corpus. Key components such as multi-head self-attention, layer normalization, and position embeddings are constructed manually to provide insight into how each part works.\n",
    "\n",
    "### What you can expect to learn\n",
    "\n",
    "- Theoretical understanding: Learn the basic building blocks of a Transformer, including attention, embeddings, and feed-forward networks.\n",
    "- Model implementation: Implement a minimal Transformer model from scratch without relying on high-level libraries.\n",
    "- Sequence modeling: Train the model to predict the next character in a sequence and use it for text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kA13w7hgpp41",
   "metadata": {
    "id": "kA13w7hgpp41"
   },
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50088839",
   "metadata": {
    "executionInfo": {
     "elapsed": 6297,
     "status": "ok",
     "timestamp": 1754488626225,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "50088839"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-FsttnMwpx_4",
   "metadata": {
    "id": "-FsttnMwpx_4"
   },
   "source": [
    "### Required Dataset\n",
    "\n",
    "A short story string is used as the dataset for training the character-level Transformer. The text is first tokenized by characters, and a vocabulary is built from all unique characters in the text.\n",
    "\n",
    "The dataset is then encoded into integer sequences and split into input-target pairs using a sliding window of fixed length. These pairs are wrapped in a custom PyTorch `Dataset` to be loaded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52fbdf5",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1754488626237,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "d52fbdf5"
   },
   "outputs": [],
   "source": [
    "text = \"Once upon a time, in a quiet village, there lived a curious cat named Momo.\\\n",
    "Every day, Momo wandered through fields and forests, chasing butterflies and asking questions.\\\n",
    "One day, Momo found a shiny key buried under a tree.\\\n",
    "She wondered, What does this key open? So, her adventure began. \"\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "\n",
    "def decode(l):\n",
    "    return \"\".join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx : idx + self.block_size + 1]\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "\n",
    "train_ds = CharDataset(data, block_size)\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VAuU2CbKqAwq",
   "metadata": {
    "id": "VAuU2CbKqAwq"
   },
   "source": [
    "### Self-Attention Layer\n",
    "\n",
    "This module implements multi-head self-attention from scratch. It projects the input sequence into query, key, and value vectors, then computes scaled dot-product attention. A causal mask is applied to prevent attending to future tokens, making it suitable for autoregressive tasks. The attention output is then combined and projected back to the original embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a48482c",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1754488626264,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "0a48482c"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device))\n",
    "        att = att.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        out = att @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fYBE4RqHh2",
   "metadata": {
    "id": "88fYBE4RqHh2"
   },
   "source": [
    "### Transformer Block\n",
    "\n",
    "This module represents a basic Transformer block consisting of a self-attention layer followed by a feed-forward neural network. Layer normalization is applied before each sub-layer, and residual connections are added after both the attention and the feed-forward layers. This structure helps stabilize training and allows the model to learn complex dependencies in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44db0493-55f6-4a86-9fd2-02d2e6182019",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1754488626282,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "44db0493-55f6-4a86-9fd2-02d2e6182019"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = SelfAttention(embed_dim, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eVcWxf8SqMG4",
   "metadata": {
    "id": "eVcWxf8SqMG4"
   },
   "source": [
    "### Mini Transformer Model\n",
    "\n",
    "This is a simplified Transformer model designed for character-level language modeling. It includes token embeddings, learnable positional embeddings, a single Transformer block, and a final linear layer to project outputs to vocabulary logits.\n",
    "\n",
    "The model takes a sequence of token indices as input, adds positional information, passes it through the Transformer block, and outputs predictions for the next character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70be4731-3df6-461a-8420-ace59da6087f",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1754488626298,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "70be4731-3df6-461a-8420-ace59da6087f"
   },
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, block_size, n_heads=2, mlp_dim=64):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, block_size, embed_dim))\n",
    "        self.block = TransformerBlock(embed_dim, n_heads, mlp_dim)\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok = self.token_embed(idx)\n",
    "        x = tok + self.pos_embed[:, : tok.size(1), :]\n",
    "        x = self.block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w8lzD2oRqXwt",
   "metadata": {
    "id": "w8lzD2oRqXwt"
   },
   "source": [
    "### Training\n",
    "\n",
    "The model is trained for multiple epochs using mini-batches from the character dataset. For each batch, it computes predictions, calculates the cross-entropy loss, and updates the model parameters using backpropagation. Every 100 epochs, the current training loss is printed for monitoring progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d1bef-4b2c-4750-964e-5ef86e7b44a6",
   "metadata": {
    "executionInfo": {
     "elapsed": 7628,
     "status": "ok",
     "timestamp": 1754488633928,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "882d1bef-4b2c-4750-964e-5ef86e7b44a6"
   },
   "outputs": [],
   "source": [
    "model = MiniTransformer(vocab_size, embed_dim=32, block_size=block_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b27d6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289290,
     "status": "ok",
     "timestamp": 1754488923220,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "e3b27d6b",
    "outputId": "523a7371-4121-4089-e88b-5bffec62a611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.8620\n",
      "Epoch 100, Loss: 0.6550\n",
      "Epoch 200, Loss: 0.1257\n",
      "Epoch 300, Loss: 0.6037\n",
      "Epoch 400, Loss: 0.3725\n",
      "Epoch 500, Loss: 0.0944\n",
      "Epoch 600, Loss: 0.3961\n",
      "Epoch 700, Loss: 0.3749\n",
      "Epoch 800, Loss: 0.4886\n",
      "Epoch 900, Loss: 0.3692\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits.view(-1, vocab_size), yb.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Qz3QNByqexW",
   "metadata": {
    "id": "6Qz3QNByqexW"
   },
   "source": [
    "### Results\n",
    "\n",
    "After training, the model can be used to generate text character-by-character. Given a starting string, the model repeatedly predicts the next character by sampling from the output probability distribution. The predicted character is appended to the input, and the process continues for the desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953bf6cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1754488923299,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "953bf6cc",
    "outputId": "acd4bf99-c4f5-4fdb-a86b-cc7b1c6f7b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curious cat named Momo.Every day, Momo found a shiny key buried under a tree.She wondered, What does \n"
     ]
    }
   ],
   "source": [
    "def sample(model, start, length):\n",
    "    model.eval()\n",
    "    idx = torch.tensor([stoi[s] for s in start], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    for _ in range(length):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        next_id = torch.multinomial(F.softmax(logits[:, -1, :], dim=-1), num_samples=1)\n",
    "        idx = torch.cat((idx, next_id), dim=1)\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "\n",
    "print(sample(model, start=\"c\", length=100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_env)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
