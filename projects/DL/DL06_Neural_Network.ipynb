{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IagZMs0_qjdL"
   },
   "source": [
    "# DL06 Neural Network\n",
    "\n",
    "### Lab Description\n",
    "\n",
    "This laboratory exercise introduces the implementation of a **fully-connected neural network** from scratch using NumPy, without relying on deep learning frameworks. The lab covers every stage of building a neural network, including parameter initialization, forward propagation, loss computation, backpropagation, and parameter updates. You will apply the model to both binary and multi-class classification problems to understand how neural networks learn from data.\n",
    "\n",
    "In this hands-on lab, you will preprocess the ``Iris`` dataset for binary classification (setosa vs. versicolor) and the ``EMG`` hand-gesture dataset for multi-class classification, train a neural network on each, and evaluate the results.\n",
    "\n",
    "### What you can expect to learn\n",
    "\n",
    "- Theoretical Understanding: Understand the components and workflow of a fully-connected neural network.\n",
    "- Data Preprocessing: Prepare datasets for binary and multi-class classification tasks.\n",
    "- Implementation:  \n",
    "  - Apply He initialization for weight parameters.  \n",
    "  - Implement numerically stable sigmoid and softmax activation functions.  \n",
    "  - Use Binary Cross-Entropy (BCE) and Categorical Cross-Entropy (CCE) loss functions.  \n",
    "  - Derive and implement backpropagation for linear and activation layers.  \n",
    "  - Train and evaluate the network on different classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGrx-5LdYhkM"
   },
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1170,
     "status": "ok",
     "timestamp": 1754669569831,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "fmTH9UkeqdYf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from sklearn import datasets\n",
    "\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2FamI7SYpeM"
   },
   "source": [
    "### Required Dataset\n",
    "Below, we prepare:\n",
    "1. ``Iris`` (binary) â€” take 100 samples (setosa vs. versicolor), split into train/val.\n",
    "2. ``EMG`` (multi-class) â€” read CSVs from URLs, split into train/val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1754669569845,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "aedIt8KhYw6W",
    "outputId": "e6f26813-276f-4f33-81e8-ff62fa96d43e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris shapes:\n",
      "  X_train_bin: (4, 90)  y_train_bin: (1, 90)\n",
      "  X_val_bin:   (4, 10)  y_val_bin:   (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# ---- Iris (binary) ----\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data[:100].T\n",
    "y_iris = np.expand_dims(iris.target[:100], axis=1).T\n",
    "\n",
    "# Train/Val split: 90/10 per class (indices mimic the original script)\n",
    "X_train_bin = np.concatenate((X_iris[:, :45], X_iris[:, 50:95]), axis=1)\n",
    "y_train_bin = np.concatenate((y_iris[:, :45], y_iris[:, 50:95]), axis=1)\n",
    "X_val_bin = np.concatenate((X_iris[:, 45:50], X_iris[:, 95:]), axis=1)\n",
    "y_val_bin = np.concatenate((y_iris[:, 45:50], y_iris[:, 95:]), axis=1)\n",
    "\n",
    "print(\"Iris shapes:\")\n",
    "print(\"  X_train_bin:\", X_train_bin.shape, \" y_train_bin:\", y_train_bin.shape)\n",
    "print(\"  X_val_bin:  \", X_val_bin.shape, \" y_val_bin:  \", y_val_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1771,
     "status": "ok",
     "timestamp": 1754669571618,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "qj0vcoVpY7Zc",
    "outputId": "42aeb7b0-33d8-49aa-87f6-6403244c02a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMG shapes:\n",
      "  X_train_mc: (64, 9000)  y_train_mc: (4, 9000)\n",
      "  X_val_mc:   (64, 342)  y_val_mc:   (4, 342)\n",
      "  X_test_mc:  (64, 2336)\n"
     ]
    }
   ],
   "source": [
    "# ---- EMG (multi-class, 4 classes) ----\n",
    "# NOTE: These URLs must be accessible in your environment.\n",
    "X_train_mc = read_csv(\n",
    "    \"https://raw.githubusercontent.com/ivanlim123/Assignment-3-Backpropagation/main/X_train.csv\", header=None\n",
    ").to_numpy()\n",
    "y_train_mc = read_csv(\n",
    "    \"https://raw.githubusercontent.com/ivanlim123/Assignment-3-Backpropagation/main/y_train.csv\", header=None\n",
    ").to_numpy()\n",
    "X_test_mc = read_csv(\n",
    "    \"https://raw.githubusercontent.com/ivanlim123/Assignment-3-Backpropagation/main/X_test.csv\", header=None\n",
    ").to_numpy()\n",
    "\n",
    "# Split a validation set from the end, as in the original script\n",
    "X_val_mc = X_train_mc[:, 9000:]\n",
    "y_val_mc = y_train_mc[:, 9000:]\n",
    "X_train_mc = X_train_mc[:, :9000]\n",
    "y_train_mc = y_train_mc[:, :9000]\n",
    "\n",
    "print(\"EMG shapes:\")\n",
    "print(\"  X_train_mc:\", X_train_mc.shape, \" y_train_mc:\", y_train_mc.shape)\n",
    "print(\"  X_val_mc:  \", X_val_mc.shape, \" y_val_mc:  \", y_val_mc.shape)\n",
    "print(\"  X_test_mc: \", X_test_mc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk22XOrSWwJ1"
   },
   "source": [
    "## Forward propagation module\n",
    "### Linear Layer\n",
    "Fully-connected layers, also known as linear layers, connect every input neuron to every output neun. \\.\n",
    "A linear layer performs the transformat:\n",
    "\n",
    "â€ƒâ€ƒ$Z = WA +  b$,\n",
    "where $W$ and $b$ are the weight and bias.\n",
    "\n",
    "### Initialize parameters\n",
    "* Step1: 1-layer neural network\n",
    "Initialize weights using He initialization. \\\n",
    "He initialization helps maintain stable gradients and is commonly used in deep networks.\n",
    "\n",
    "* Step2: L-layer neural network\n",
    "Initialize parameters for an L-layer neural network with He initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1754669571668,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "x0KHo8w9yqbY"
   },
   "outputs": [],
   "source": [
    "# Function to initialize parameters for a neural network\n",
    "def initialize_parameters(n_x, n_y):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(1)\n",
    "\n",
    "    W1 = np.random.randn(n_y, n_x) * np.sqrt(2 / n_x)\n",
    "    b1 = np.zeros((n_y, 1)) * np.sqrt(2 / n_x)\n",
    "\n",
    "    assert W1.shape == (n_y, n_x)\n",
    "    assert b1.shape == (n_y, 1)\n",
    "\n",
    "    parameters = {\"W1\": W1, \"b1\": b1}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1754669571692,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "7HNAWwmg8R7T",
    "outputId": "f15ccf1c-2a6d-407c-e796-725af73ee531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.32627244 -0.49949702 -0.43125043]]\n",
      "b1 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3, 1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "\n",
    "output[\"initialize_parameters\"] = initialize_parameters(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1754669571695,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "DEAIJKJ7-iQz"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l - 1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1)) * np.sqrt(2 / layer_dims[l - 1])\n",
    "\n",
    "        assert parameters[\"W\" + str(l)].shape == (layer_dims[l], layer_dims[l - 1])\n",
    "        assert parameters[\"b\" + str(l)].shape == (layer_dims[l], 1)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1754669571708,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "zPSp12IZ_j38",
    "outputId": "009c65a0-a834-4883-ea03-4e767ca6316b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.02732621 -0.38690873 -0.33404515 -0.67860494  0.54733184]\n",
      " [-1.45562088  1.10351585 -0.48142952  0.20177804 -0.15771567]\n",
      " [ 0.92471825 -1.30294739 -0.20391454 -0.2428973   0.71705876]\n",
      " [-0.69563232 -0.10905317 -0.55520641  0.02669832  0.36860471]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.77825528  0.8094419   0.63752091  0.35531715]\n",
      " [ 0.63700135 -0.48346861 -0.08689651 -0.66168891]\n",
      " [-0.18942548  0.37501795 -0.48907801 -0.28054711]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5, 4, 3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "output[\"initialize_parameters_deep\"] = initialize_parameters_deep([3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abu7YqxeAeMz"
   },
   "source": [
    "### Linear Forward\n",
    "\n",
    "The linear forward step computes the output of a layer by applying a linear transformation to the input. Specifically, it takes the input activations from the previous layer and calculates: $Z = WA + b$. This step captures the weighted combination of inputs plus a bias term, preparing the data for the next layer or activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754669571711,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "O_oGueTE8X61"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = W.dot(A) + b\n",
    "\n",
    "    assert Z.shape == (W.shape[0], A.shape[1])\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1754669571736,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "SSf8JIyjaj_A",
    "outputId": "2882ceef-de37-49da-80ab-62328fbd00dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[1.9 2.2 2.5]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = np.array([[0, 0.5, 1], [1, 1.5, 2], [2, 2.5, 3]]), np.array([[0.1, 0.2, 0.3]]), np.array([[1.1]])\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))\n",
    "\n",
    "output[\"linear_forward\"] = linear_forward(\n",
    "    np.array([[0, -0.5, -1], [1, 1.5, 2], [-2, -2.5, -3]]), np.array([[0.5, 0.3, 0.7]]), np.array([[-1.1]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PkLKaFWiWmF"
   },
   "source": [
    "### Activation Function Layer\n",
    "\n",
    "In this part, you'll implement commonly used **activation functions**, which add non-linearity to the network. We'll focus on three important types: **Sigmoid**, **Softmax**, and **ReLU**.\n",
    "\n",
    "#### 1. **Sigmoid Function**\n",
    "\n",
    "The **sigmoid** function maps any input to a value between 0 and 1, which makes it useful for binary classification:\n",
    "\n",
    "$$\n",
    "\\sigma(Z) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{1 + e^{-Z}}, & \\text{if } Z \\geq 0 \\\\\n",
    "\\frac{e^{Z}}{1 + e^{Z}}, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "âš ï¸ To avoid overflow when computing exponentials on large negative or positive numbers, we use **numerically stable version**.\n",
    "\n",
    "#### 2. **Softmax Function**\n",
    "\n",
    "The **softmax** function turns a vector of values into probabilities that sum to 1, commonly used for multi-class classification:\n",
    "\n",
    "$$\n",
    "\\sigma(\\vec{Z})_i = \\frac{e^{Z_i - b}}{\\sum_{j=1}^{K} e^{Z_j - b}}, \\quad b = \\max_j Z_j\n",
    "$$\n",
    "\n",
    "âš ï¸ Without this trick, large values of \\$Z\\$ can cause overflow errors (e.g., warnings like `overflow encountered in exp`). This implementation ensures numerical safety.\n",
    "\n",
    "#### 3. **ReLU Function**\n",
    "\n",
    "The **ReLU (Rectified Linear Unit)** is one of the most popular activation functions in deep learning. It simply outputs the input if itâ€™s positive, and 0 otherwise:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(Z) = \\max(Z, 0)\n",
    "$$\n",
    "\n",
    "It's simple and efficient, helping networks learn faster and avoid vanishing gradients.\n",
    "\n",
    "Each activation function plays a key role depending on the task:\n",
    "\n",
    "* Use **Sigmoid** for binary outputs\n",
    "* Use **Softmax** for multi-class probabilities\n",
    "* Use **ReLU** in hidden layers for speed and effectiveness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1754669571754,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "Nnuv8MmebMgg"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    new_Z = Z.ravel()\n",
    "    A = []\n",
    "    for i in new_Z:\n",
    "        if i >= 0:\n",
    "            A.append(1 / (1 + np.exp(-i)))\n",
    "        else:\n",
    "            A.append(np.exp(i) / (1 + np.exp(i)))\n",
    "    np.array(A).reshape(Z.shape)\n",
    "    A = np.array([A])\n",
    "\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    max = np.max(Z)\n",
    "    A = np.exp(Z - max) / sum(np.exp(Z - max))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "\n",
    "    assert A.shape == Z.shape\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1754669571785,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "gBuRAoeUC5jV",
    "outputId": "fcc0b4cc-f6c5-446f-cf68-b9a2368fc3b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: A = [[0.00669285 0.26894142 0.5        0.73105858 0.99330715]]\n",
      "ReLU: A = [[0 0 0 1 5]]\n",
      "Softmax: A = \n",
      "[[0.0320586  0.1748777  0.0320586 ]\n",
      " [0.08714432 0.47536689 0.08714432]\n",
      " [0.23688282 0.1748777  0.23688282]\n",
      " [0.64391426 0.1748777  0.64391426]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([[-5, -1, 0, 1, 5]])\n",
    "A, cache = sigmoid(Z)\n",
    "print(\"Sigmoid: A = \" + str(A))\n",
    "output[\"sigmoid\"] = sigmoid(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
    "\n",
    "A, cache = relu(Z)\n",
    "print(\"ReLU: A = \" + str(A))\n",
    "output[\"relu\"] = relu(np.array([[-1.82, -0.71, 0.02, 0.13, 2.21]]))\n",
    "\n",
    "\n",
    "Z = np.array([[1, 0, -2], [2, 1, -1], [3, 0, 0], [4, 0, 1]])\n",
    "A, cache = softmax(Z)\n",
    "print(\"Softmax: A = \\n\" + str(A))\n",
    "output[\"softmax\"] = softmax(np.array([[0.1, 1.2, -2.1], [2.2, 0.7, -1.3], [1.4, 0.3, 0.2], [3.9, 0.5, -1.6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nN34fUuWwJ6"
   },
   "source": [
    "### Model forward\n",
    "Now we can start to build a forward model.\n",
    "\n",
    "#### 1. Linear activation forward\n",
    "To begin with, we should merge the linear transformation and activation function into a single linear-activation layer, as defined by the equation:\n",
    "$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} + b^{[l]})$,\n",
    "where $g$ represents the activation function, such as `sigmoid()`, `softmax()`, or `relu()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1754669571788,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "0JGMzfIDCSVz"
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "\n",
    "    assert A.shape == (W.shape[0], A_prev.shape[1])\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1754669571848,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "6yVQQqe2EyHA",
    "outputId": "8a3b6e73-1ffc-4e56-beee-dfe01700a96d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.64565631 0.20915937 0.77902611]]\n",
      "With ReLU: A = [[0.6  0.   1.26]]\n",
      "With softmax: A = \n",
      "[[0.47535001 0.05272708 0.68692136]\n",
      " [0.14317267 0.75380161 0.05526942]\n",
      " [0.38147732 0.19347131 0.25780921]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = (\n",
    "    np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]),\n",
    "    np.array([[0.1, 0.2, 0.3]]),\n",
    "    np.array([[-0.5]]),\n",
    ")\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "\n",
    "A_prev, W, b = (\n",
    "    np.array([[0.1, -1.2, 1.9], [1.1, 0.2, 2.3], [2.9, -2.5, 3.7]]),\n",
    "    np.array([[0.1, 0.2, 0.3], [-0.1, -0.2, -0.3], [-0.1, 0, 0.1]]),\n",
    "    np.array([[-0.5], [0.5], [0.1]]),\n",
    ")\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation=\"softmax\")\n",
    "print(\"With softmax: A = \\n\" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJVlZeyNAu-y"
   },
   "source": [
    "#### 2. L model forward\n",
    "For even more convenience when implementing the $L$-layer Neural Net, you will need a function that replicates the previous one (linear_activation_forward with `relu()`) $L-1$ times, then follows that with one linear_activation_forward with `sigmoid()` for binary classification or `softmax()` for multi-class classification.\n",
    "\n",
    "In the code below, the variable AL will denote $A^{[L]} = g(Z^{[L]}) = g(W^{[L]}A^{[L-1]}+b^{[L]})$, where activation function g could be `sigmoid()` for binary classification or `softmax()` for multi-class classification.\n",
    "\n",
    "**Instruction**:\n",
    "*   Use the functions you had previously written.\n",
    "*   Use a for loop to replicate [LINEAR->RELU] (L-1) times.\n",
    "*   Don't forget to keep track of the caches in the \"caches\" list. To add a new value c to a list, you can use list.append(c).\n",
    "\n",
    "**Note**: There are N nodes in the last layer for N-class classification, but only one node for binary classification. Intuitively, this could be pretty confusing sometimes since there should be two nodes in the last layer for binary classification. However, both the one-node(sigmoid, binary cross-entropy) and two-node(softmax, categorical cross-entropy) techniques for binary classification work fine, and picking one technique over the other is a matter of subjective preference. For this assignment, you will implement the former one, which is what we usually do for binary classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754669571851,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "k9jRgHX-FTpQ"
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, classes):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    if classes == 2:\n",
    "        AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "        assert AL.shape == (1, X.shape[1])\n",
    "    else:\n",
    "        AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"softmax\")\n",
    "        caches.append(cache)\n",
    "        assert AL.shape == (classes, X.shape[1])\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1754669571860,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "s26LVkCbIbJ3",
    "outputId": "37bb751c-225b-4186-9bcc-797b282464e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.07802314 0.01818575 0.00403778]]\n",
      "Length of caches list = 2\n",
      "AL = [[6.18255541e-03 7.61028230e-04 7.66374121e-05]\n",
      " [2.11940510e-01 2.01651685e-01 1.56962913e-01]\n",
      " [1.35096366e-02 2.03338206e-03 2.50381145e-04]\n",
      " [2.18485590e-02 6.14321609e-03 1.41311042e-03]\n",
      " [2.66899863e-01 4.51947825e-01 6.26088267e-01]\n",
      " [1.13256434e-01 1.07946425e-01 8.41707040e-02]\n",
      " [7.27765014e-02 3.08118586e-02 1.06721503e-02]\n",
      " [2.17220658e-02 4.88048793e-03 8.97083101e-04]\n",
      " [4.19482610e-02 1.32235761e-02 3.41028905e-03]\n",
      " [2.29915615e-01 1.80600516e-01 1.16058465e-01]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "# binary classification\n",
    "X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3, 3, 1]), 2\n",
    "AL, caches = L_model_forward(X, parameters, classes)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))\n",
    "\n",
    "# multi-class classification\n",
    "X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3, 3, 10]), 10\n",
    "AL, caches = L_model_forward(X, parameters, classes)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSBVaQOSRrk"
   },
   "source": [
    "### Cost function\n",
    "In this section, you will implement the cost function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification. You need to compute the cost, because you want to check if your model is actually learning. Cross-entropy loss is minimized, where smaller values represent a better model than larger values. A model that predicts perfect probabilities has a cross entropy or log loss of 0.0.\n",
    "\n",
    "#### 1. Binary cross-entropy loss\n",
    "**Exercise**: Compute the binary cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1754669571885,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "MjBT0eYQaY81"
   },
   "outputs": [],
   "source": [
    "def compute_BCE_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1 / m) * (-np.dot(Y, np.log(AL).T) - np.dot(1 - Y, np.log(1 - AL).T))\n",
    "\n",
    "    cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert cost.shape == ()\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1754669571895,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "r07sqnIXaaMv",
    "outputId": "9d75af98-4a1f-4341-e704-6f5fb4c98295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.5784031417840181\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.9, 0.6, 0.4, 0.1, 0.2, 0.8]]), np.array([[1, 1, 1, 0, 0, 0]])\n",
    "\n",
    "print(\"cost = \" + str(compute_BCE_cost(AL, Y)))\n",
    "output[\"compute_BCE_cost\"] = compute_BCE_cost(\n",
    "    np.array([[0.791, 0.983, 0.654, 0.102, 0.212, 0.091, 0.476, 0.899]]), np.array([[1, 1, 1, 1, 0, 0, 0, 0]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aealRyKbcQzG"
   },
   "source": [
    "#### 2. Categorical cross-entropy loss\n",
    "**Exercise**: Compute the categorical cross-entropy cost $J$, using the following formula: (5%) $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1754669571942,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "Owx-kTdcfxV5"
   },
   "outputs": [],
   "source": [
    "def compute_CCE_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -(1.0 / m) * np.sum(Y * np.log(AL))\n",
    "\n",
    "    cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert cost.shape == ()\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1754669571943,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "0YbHVAc7hSh3",
    "outputId": "fa5e5693-fb7e-4feb-d0e9-67f880e6fb99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.472268904012926\n"
     ]
    }
   ],
   "source": [
    "AL, Y = (\n",
    "    np.array([[0.8, 0.6, 0.4, 0.1, 0.2, 0.4], [0.1, 0.3, 0.5, 0.7, 0.1, 0.1], [0.1, 0.1, 0.1, 0.2, 0.7, 0.5]]),\n",
    "    np.array([[1, 1, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 1, 1]]),\n",
    ")\n",
    "print(\"cost = \" + str(compute_CCE_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuplx8dBeZtO"
   },
   "source": [
    "### Backward propagation module\n",
    "In this section, you will implement helper functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "A quick revision for backpropapagation:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/6ebc4d60a90c1592aee3eaa3113eb4b37d9b4b19/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/images/backprop_kiank.png\" height=\"200\"/>\n",
    "\n",
    "Similar to forward propagation, you are going to build the backward propagation in three steps:\n",
    "*   LINEAR backward\n",
    "*   LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU, sigmoid or softmax activation\n",
    "*   [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID/SOFTMAX backward (whole model)\n",
    "\n",
    "#### 1. Linear backward\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n",
    "\n",
    "**Exercise**: Use the 3 formulas above to implement `linear_backward()`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1754669571943,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "k5HL2LG6eeVn"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    # Here cache is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1754669571944,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "QJfNFIbF4RvH",
    "outputId": "c1d27212-b6f2-4885-8666-536b7704217d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[3.5 6. ]]\n",
      "dW = [[1.625]\n",
      " [0.625]]\n",
      "db = [[2.  ]\n",
      " [0.75]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = (\n",
    "    np.array([[1.5, 2.5], [0.5, 1.0]]),\n",
    "    (np.array([[0.5, 1]]), np.array([[2.0], [1.0]]), np.array([[0.5], [1.0]])),\n",
    ")\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print(\"dA_prev = \" + str(dA_prev))\n",
    "print(\"dW = \" + str(dW))\n",
    "print(\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3CBdcyZJHdF"
   },
   "source": [
    "#### 2. Linear-Activation backward\n",
    "Next, you will create a function that merges the two helper functions: `linear_backward()` and the backward step for the activation `linear_activation_backward()`.\n",
    "\n",
    "First, you will need to implement the backward functions of `sigmoid()`, `relu()` and `softmax()`+`compute_CCE_cost`.\n",
    "\n",
    "**Exercise**: Implement backward function. (10%+5%) (basic: Sigmoid and ReLU, bonus: Softmax+CCE_loss)\n",
    "\n",
    "**Instruction**:\n",
    "*   sigmoid_backward: Implements the backward propagation for SIGMOID unit.\n",
    "*   relu_backward: Implements the backward propagation for RELU unit.\n",
    "*   softmax_CCE_backward: Implements the backward propagation for [SOFTMAX->LOSS] unit.\n",
    "\n",
    "If $g(.)$ is the activation function, sigmoid_backward, relu_backward and softmax_backward compute$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "\n",
    "1. The derivative of the sigmoid function is: $$Ïƒ^{'}(Z^{[l]}) = Ïƒ(Z^{[l]}) (1 - Ïƒ(Z^{[l]}))$$. <br>\n",
    "â—**Important**â—: You should use the numerically stable sigmoid function to prevent the overflow exponential problem.\n",
    "\n",
    "2. The derivative of the relu function is: $$g'(Z^{[l]}) = \\begin{cases}\n",
    "    1,& \\text{if } Z^{[l]}> 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "3. TLDRðŸ˜‰: The derivative of the categorical cross-entropy loss with respect to the last hidden layer is: $$\\frac{\\partial \\mathcal{L}}{\\partial Z} = s - y $$. <br> The derivative of the softmax function is: $$\\frac{\\partial S(z_i)}{\\partial z_j} = \\begin{cases}\n",
    "    S(z_i) \\times (1 - S(z_i)),& \\text{if } i = j\\\\\n",
    "    -S(z_i) \\times S(z_j),              & \\text{if } i \\neq j\n",
    "\\end{cases}$$, where $z$ is a vector with shape (number of classes K, 1) and $S(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$. Hence, the real derivative of softmax function would be a full Jacobian matrix. For the special case, K = 4, we have <img src=\"https://miro.medium.com/max/554/1*SWfgFQLDIPXDf1C6CHmr8A.png\" height=\"100\"/>. <br> It is quite complicated to calculate the softmax derivative on its own. However, if you use the softmax and the cross entropy loss, that complexity fades away. Since the softmax layer is usually used at the output, we can actually calculate the derivative of the categorical cross-entropy loss with respect to the n-th node in the last hidden layer. Instead of a long clunky formula, you end up with this terse, easy to compute thing: $$\\frac{\\partial \\mathcal{L}}{\\partial Z_i} = s_i - y_i $$, where $s$ is the output of the softmax function and the $y$ is the true label vector(one-hot vector). For more information, you can refer to this article [Derivative of the Softmax Function and the Categorical Cross-Entropy Loss](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1). <br>\n",
    "â—**Important**â—: The above mathematical derivation is based on naive implementation. In order to deal with the exponential overflow problem, we should use the normalized exponential function when counting $s$. For the sake of simplicity, we just use the same gradient equation as the naive implementation.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1754669571944,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "m9gGiDslLqZr"
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    new_Z = Z.ravel()\n",
    "    A = []\n",
    "    for i in new_Z:\n",
    "        if i >= 0:\n",
    "            A.append(1 / (1 + np.exp(-i)))\n",
    "        else:\n",
    "            A.append(np.exp(i) / (1 + np.exp(i)))\n",
    "    np.array(A).reshape(Z.shape)\n",
    "    A = np.array([A])\n",
    "    dZ = dA * A * (1 - A)\n",
    "\n",
    "    assert dZ.shape == Z.shape\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "    dZ[Z <= 0] = 0  # When z <= 0, you should set dz to 0 as well.\n",
    "\n",
    "    assert dZ.shape == Z.shape\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def softmax_CCE_backward(Y, cache):\n",
    "    Z = cache\n",
    "    max = np.max(Z)\n",
    "    A = np.exp(Z - max) / sum(np.exp(Z - max))\n",
    "    dZ = A - Y\n",
    "\n",
    "    assert dZ.shape == Z.shape\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1754669571998,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "z0OG6KtNyYZY",
    "outputId": "a5886c69-3b8b-4b4c-e21e-e63b0e6a815d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: dZ = [[-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]\n",
      "ReLU: dZ = [[ 0.    0.   -1.14]\n",
      " [ 1.7   0.    3.72]]\n",
      "Softmax: dZ = [[-0.96488097  0.09003057  0.01766842]\n",
      " [ 0.70538451 -0.75527153  0.01766842]\n",
      " [ 0.25949646  0.66524096 -0.03533684]]\n"
     ]
    }
   ],
   "source": [
    "dA, cache = np.array([[-2, -1.37, -1.14, -2, -3.72]]), np.array([[0, 1, 2, 0, 1]])\n",
    "dZ = sigmoid_backward(dA, cache)\n",
    "print(\"Sigmoid: dZ = \" + str(dZ))\n",
    "\n",
    "dA, cache = np.array([[-2, -1.37, -1.14], [1.7, 2, 3.72]]), np.array([[-2, -1, 2], [1, 0, 1]])\n",
    "dZ = relu_backward(dA, cache)\n",
    "print(\"ReLU: dZ = \" + str(dZ))\n",
    "\n",
    "Y, cache = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), np.array([[-2, -1, -2], [1, 0, -2], [0, 1, 2]])\n",
    "dZ = softmax_CCE_backward(Y, cache)\n",
    "print(\"Softmax: dZ = \" + str(dZ))\n",
    "output[\"softmax_CCE_backward\"] = softmax_CCE_backward(\n",
    "    np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n",
    "    np.array([[-2.11, -1.22, -2.33], [1.44, 0.55, -2.66], [0.77, 1.88, 2.99]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1754669571999,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "CTwbEEG14Emy"
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1754669572000,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "odZeIDj942uR",
    "outputId": "790520fa-8b95-4978-e3e8-47ce8872f574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[-1.         -0.5387167  -0.23938537 -1.         -1.46279278]\n",
      " [-0.5        -0.26935835 -0.11969269 -0.5        -0.73139639]]\n",
      "dW = [[-0.13868689  0.13868689]]\n",
      "db = [[-0.42408949]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[3.4  0.   9.72]\n",
      " [1.7  0.   4.86]\n",
      " [0.   0.   0.  ]]\n",
      "dW = [[ 0.          0.         -0.76      ]\n",
      " [-1.13333333  1.13333333  2.48      ]]\n",
      "db = [[-0.38      ]\n",
      " [ 1.80666667]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = (\n",
    "    np.array([[-2, -1.37, -1.14, -2, -3.72]]),\n",
    "    (\n",
    "        (np.array([[-2, -1, 0, 1, 2], [2, 1, 0, -1, -2]]), np.array([[2.0, 1.0]]), np.array([[0.5]])),\n",
    "        np.array([[0, 1, 2, 0, 1]]),\n",
    "    ),\n",
    ")\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation=\"sigmoid\")\n",
    "print(\"sigmoid:\")\n",
    "print(\"dA_prev = \" + str(dA_prev))\n",
    "print(\"dW = \" + str(dW))\n",
    "print(\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dAL, linear_activation_cache = (\n",
    "    np.array([[-2, -1.37, -1.14], [1.7, 2, 3.72]]),\n",
    "    (\n",
    "        (np.array([[-2, -1, 0], [2, 1, 0], [0, 1, 2]]), np.array([[-2, -1, 0], [2, 1, 0]]), np.array([[0.5], [-0.5]])),\n",
    "        np.array([[-2, -1, 2], [1, 0, 1]]),\n",
    "    ),\n",
    ")\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation=\"relu\")\n",
    "print(\"relu:\")\n",
    "print(\"dA_prev = \" + str(dA_prev))\n",
    "print(\"dW = \" + str(dW))\n",
    "print(\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOFlw70x88A6"
   },
   "source": [
    "#### 3 - L-Model Backward\n",
    "Now you will implement the backward function for the whole network. Recall that when you implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the `L_model_backward` function, you will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$. Figure below shows the backward pass.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amanchadha/coursera-deep-learning-specialization/6ebc4d60a90c1592aee3eaa3113eb4b37d9b4b19/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/images/mn_backward.png\" height=\"400\"/>\n",
    "\n",
    "Initializing backpropagation: To backpropagate through this network, we know that the output is, $A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute dAL $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$. To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
    "```\n",
    "dAL = - (np.divide(Y, AL + Ïµ) - np.divide(1 - Y, 1 - AL + Ïµ)) # derivative of cost with respect to AL, where Ïµ = 1e-5 is added to prevent zero division.\n",
    "```\n",
    "\n",
    "\n",
    "You can then use this post-activation gradient dAL to keep going backward. As seen in figure above, you can now feed in dAL into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a for loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :\n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]} $$\n",
    "For example, for $l=3$ this would store $dW^{[l]}$ in grads[\"dW3\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1754669572000,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "IPnv1BZd_GfF"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches, classes):\n",
    "    grads = {}\n",
    "    L = len(caches)  # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)  # after this line, Y is the same shape as AL\n",
    "\n",
    "    if classes == 2:\n",
    "        # Initializing the backpropagation\n",
    "        dAL = -(np.divide(Y, AL + 1e-5) - np.divide(1 - Y, 1 - AL + 1e-5))\n",
    "\n",
    "        # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        current_cache = caches[L - 1]  # Last Layer\n",
    "        grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "            dAL, current_cache, \"sigmoid\"\n",
    "        )\n",
    "    else:\n",
    "        # Initializing the backpropagation\n",
    "        linear_cache, activation_cache = caches[L - 1]  # Last Layer\n",
    "        dZ = softmax_CCE_backward(Y, activation_cache)\n",
    "\n",
    "        # Lth layer (LINEAR) gradients. Inputs: \"dZ, linear_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
    "            grads[\"dA\" + str(l + 1)], current_cache, activation=\"relu\"\n",
    "        )\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1754671317085,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "9jaRO9SvCdEY",
    "outputId": "785cef3e-647d-4acb-b196-4ff6449cbd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification\n",
      "dW1 = [[-0.00178234 -0.12389888  0.03052914]\n",
      " [ 0.          0.          0.        ]\n",
      " [-0.01472455 -1.0235744   0.25221246]]\n",
      "db1 = [[0.06105827]\n",
      " [0.        ]\n",
      " [0.50442493]]\n",
      "dA1 = [[ 1.87699706e-01 -3.70276293e-03 -8.22125021e-04]\n",
      " [-1.10052058e+00  2.17100329e-02  4.82028195e-03]\n",
      " [ 1.55065655e+00 -3.05898912e-02 -6.79187823e-03]]\n",
      "\n",
      "Multi-class classification\n",
      "dW1 = [[ 0.42051843  0.06541362  0.0887762 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.08372031 -1.61959581  0.67582903]]\n",
      "db1 = [[0.1775524 ]\n",
      " [0.        ]\n",
      " [1.35165806]]\n",
      "dA1 = [[-0.09507462 -0.00609164  0.63382346]\n",
      " [-1.45707343  0.00732963 -0.17178251]\n",
      " [ 2.4424153  -0.02604318  1.63860206]]\n"
     ]
    }
   ],
   "source": [
    "# binary classification\n",
    "X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3, 3, 1]), 2\n",
    "AL, caches = L_model_forward(X, parameters, classes)\n",
    "Y_assess = np.array([[1, 0, 0]])\n",
    "grads = L_model_backward(AL, Y_assess, caches, classes)\n",
    "print(\"Binary classification\")\n",
    "print(\"dW1 = \" + str(grads[\"dW1\"]))\n",
    "print(\"db1 = \" + str(grads[\"db1\"]))\n",
    "print(\"dA1 = \" + str(grads[\"dA1\"]) + \"\\n\")\n",
    "\n",
    "# multi-class classification\n",
    "X, parameters, classes = np.array([[0, 1, 2], [-2, -1, 0], [0.5, 0.5, 0.5]]), initialize_parameters_deep([3, 3, 3]), 3\n",
    "AL, caches = L_model_forward(X, parameters, classes)\n",
    "Y_assess = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "grads = L_model_backward(AL, Y_assess, caches, classes)\n",
    "print(\"Multi-class classification\")\n",
    "print(\"dW1 = \" + str(grads[\"dW1\"]))\n",
    "print(\"db1 = \" + str(grads[\"db1\"]))\n",
    "print(\"dA1 = \" + str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MkxS_itFkV3"
   },
   "source": [
    "### Update parameters\n",
    "In this section you will update the parameters of the model, using gradient descent:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary.\n",
    "\n",
    "**Exercise**: Implement update_parameters() to update your parameters using gradient descent.\n",
    "\n",
    "**Instructions**:\n",
    "*   Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1754669886886,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "GZZtqaXlFpYP"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1754669888522,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "qfKajEa9HjDO",
    "outputId": "9a8dc2d1-d820-43a0-a050-cd46b67b9509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.39721186  0.64025004 -0.09671178  0.27099015]\n",
      " [ 0.07752363  0.00469968  0.09679955  0.33705631]\n",
      " [ 0.392862    0.52183369  0.33138026  0.67538482]]\n",
      "b1 = [[ 0.16234149]\n",
      " [ 0.78232848]\n",
      " [-0.02592894]]\n",
      "W2 = [[0.6012798  0.38575324 0.49003974]]\n",
      "b2 = [[0.05692437]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters, grads = (\n",
    "    {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(3, 1), \"W2\": np.random.rand(1, 3), \"b2\": np.random.rand(1, 1)},\n",
    "    {\n",
    "        \"dW1\": np.random.rand(3, 4),\n",
    "        \"db1\": np.random.rand(3, 1),\n",
    "        \"dW2\": np.random.rand(1, 3),\n",
    "        \"db2\": np.random.rand(1, 1),\n",
    "    },\n",
    ")\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "output[\"update_parameters\"] = update_parameters(\n",
    "    {\n",
    "        \"W1\": np.random.randn(3, 4),\n",
    "        \"b1\": np.random.randn(3, 1),\n",
    "        \"W2\": np.random.randn(1, 3),\n",
    "        \"b2\": np.random.randn(1, 1),\n",
    "    },\n",
    "    {\n",
    "        \"dW1\": np.random.randn(3, 4),\n",
    "        \"db1\": np.random.randn(3, 1),\n",
    "        \"dW2\": np.random.randn(1, 3),\n",
    "        \"db2\": np.random.randn(1, 1),\n",
    "    },\n",
    "    0.075,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpQah0JDdMyl"
   },
   "source": [
    "### Basic implementation (binary classification)\n",
    "\n",
    "Congratulations on implementing all the functions by yourself. You have done an incredible job! ðŸ‘\n",
    "\n",
    "Now you have all the tools you need to get started with classification. In this section, you will build a binary classifier using the functions you had previously written. You will create a model that can classify the different species of the Iris flower. The Iris dataset consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. However, you only need to classify Iris setosa and Iris versicolor in this exercise.\n",
    "\n",
    "**Exercise**: Implement a binary classifier and tune hyperparameter.\n",
    "\n",
    "**Instruction**:\n",
    "*   Train a model with validation accuracy higher than 80%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1754669891516,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "fI7JY5ESjhZ2"
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False, classes=2):\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "\n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID/SOFTMAX.\n",
    "        AL, caches = L_model_forward(X, parameters, classes)\n",
    "\n",
    "        # Compute cost.\n",
    "        if classes == 2:\n",
    "            cost = compute_BCE_cost(AL, Y)\n",
    "        else:\n",
    "            cost = compute_CCE_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches, classes)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel(\"cost\")\n",
    "    plt.xlabel(\"iterations (per hundreds)\")\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh-9LJaBczQc"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 993
    },
    "executionInfo": {
     "elapsed": 1199,
     "status": "ok",
     "timestamp": 1754669896175,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "ZDYhhVTDwA8K",
    "outputId": "f0eafa0e-daa8-4234-fa17-655b13bb5fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.808758\n",
      "Cost after iteration 100: 0.646826\n",
      "Cost after iteration 200: 0.423764\n",
      "Cost after iteration 300: 0.304835\n",
      "Cost after iteration 400: 0.234744\n",
      "Cost after iteration 500: 0.189654\n",
      "Cost after iteration 600: 0.158602\n",
      "Cost after iteration 700: 0.136072\n",
      "Cost after iteration 800: 0.119049\n",
      "Cost after iteration 900: 0.105767\n",
      "Cost after iteration 1000: 0.095133\n",
      "Cost after iteration 1100: 0.086437\n",
      "Cost after iteration 1200: 0.079198\n",
      "Cost after iteration 1300: 0.073081\n",
      "Cost after iteration 1400: 0.067847\n",
      "Cost after iteration 1500: 0.063319\n",
      "Cost after iteration 1600: 0.059362\n",
      "Cost after iteration 1700: 0.055877\n",
      "Cost after iteration 1800: 0.052783\n",
      "Cost after iteration 1900: 0.050019\n",
      "Cost after iteration 2000: 0.047534\n",
      "Cost after iteration 2100: 0.045289\n",
      "Cost after iteration 2200: 0.043250\n",
      "Cost after iteration 2300: 0.041390\n",
      "Cost after iteration 2400: 0.039686\n",
      "Cost after iteration 2500: 0.038120\n",
      "Cost after iteration 2600: 0.036676\n",
      "Cost after iteration 2700: 0.035339\n",
      "Cost after iteration 2800: 0.034099\n",
      "Cost after iteration 2900: 0.032944\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Iris Dataset\n",
    "layers_dims = [4, 1]\n",
    "parameters = L_layer_model(\n",
    "    X_train_bin, y_train_bin, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=True, classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOV9DuFYc6Kj"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1754669900287,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "vsx_EzPf58rf"
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, classes):\n",
    "    \"\"\"\n",
    "    Predicts the output of a neural network given input data and parameters.\n",
    "    Args:\n",
    "        X: Input data (features).\n",
    "        y: True labels (optional, for accuracy calculation).\n",
    "        parameters: Parameters of the neural network.\n",
    "        classes: Number of classes in the output, 2 for binary classification, greater than 2 for multi-class classification.\n",
    "    Returns:\n",
    "        p: predictions for the given dataset X.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Initialize predictions\n",
    "    p = np.zeros((1, m)) if classes == 2 else np.zeros((classes, m))\n",
    "\n",
    "    # Forward propagation\n",
    "    probas, _ = L_model_forward(X, parameters, classes)\n",
    "\n",
    "    if classes == 2:\n",
    "        # Binary classification: convert probabilities to 0/1 predictions\n",
    "        p[0, :] = (probas[0, :] > 0.5).astype(int)\n",
    "\n",
    "        # Print accuracy if true labels are provided\n",
    "        if y is not None:\n",
    "            accuracy = np.sum((p == y) / m)\n",
    "            print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    else:\n",
    "        # Multi-class classification: convert probabilities to one-hot predictions\n",
    "        predictions = np.argmax(probas, axis=0)\n",
    "        p[predictions, np.arange(m)] = 1\n",
    "\n",
    "        # Print accuracy if true labels are provided\n",
    "        if y is not None:\n",
    "            correct_predictions = np.sum(np.all(p == y, axis=0))\n",
    "            accuracy = correct_predictions / m\n",
    "            print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1754669902392,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "xkeoJrFZznMf",
    "outputId": "54ed9ab4-cb72-4efd-8388-b172b489ad7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(X_train_bin, y_train_bin, parameters, 2)\n",
    "pred_val = predict(X_val_bin, y_val_bin, parameters, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMCpPFMVdj36"
   },
   "source": [
    "### Multi class classification\n",
    "\n",
    "In this section, you need to implement a multi-class classifier using the functions you had previously written. You will create a model that can classify four hand gestures using electromyography (EMG) signals. The EMG signal is a biomedical signal that measures electrical currents generated in muscles during its contraction representing neuromuscular activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PIT09ZPc_60"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1410567,
     "status": "ok",
     "timestamp": 1754671316981,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "HYD-qRs7doU0",
    "outputId": "3cb94f52-9efd-425e-91c5-5b63e15363d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.067374\n",
      "Cost after iteration 100: 1.344188\n",
      "Cost after iteration 200: 1.246079\n",
      "Cost after iteration 300: 1.166323\n",
      "Cost after iteration 400: 1.087787\n",
      "Cost after iteration 500: 1.010922\n",
      "Cost after iteration 600: 0.937790\n",
      "Cost after iteration 700: 0.869723\n",
      "Cost after iteration 800: 0.807598\n",
      "Cost after iteration 900: 0.751395\n",
      "Cost after iteration 1000: 0.700703\n",
      "Cost after iteration 1100: 0.654597\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [64, 256, 32, 256, 4]\n",
    "parameters = L_layer_model(\n",
    "    X_train_mc, y_train_mc, layers_dims, learning_rate=0.0075, num_iterations=8000, print_cost=True, classes=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjmeSbyKdCcy"
   },
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1754671317061,
     "user": {
      "displayName": "LIAO FLORA",
      "userId": "18011015488649422122"
     },
     "user_tz": -480
    },
    "id": "yI92fh4JXC1k",
    "outputId": "cefac641-463e-4d42-c26f-4ca2cf584eb4"
   },
   "outputs": [],
   "source": [
    "pred_train = predict(X_train_mc, y_train_mc, parameters, 4)\n",
    "pred_val = predict(X_val_mc, y_val_mc, parameters, 4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_env)",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
