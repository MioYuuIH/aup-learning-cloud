{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Neural Network Fundamentals - Linear Layers and Tensor Operations\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This lab explores the fundamental building blocks of neural networks, focusing on linear transformations, tensor operations, and the mathematical foundations that power transformer architectures.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand linear layers and their role in neural networks\n",
    "- Master tensor operations and broadcasting in PyTorch\n",
    "- Learn about weight matrices and bias vectors\n",
    "- Explore matrix multiplication in the context of neural networks\n",
    "- Understand how linear transformations work in transformer models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.1+rocm7.10.0\n",
      "GPU: Radeon 8060S Graphics\n",
      "GPU Memory: 103.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Initialize AMD GPU Backend and Required Libraries\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Core libraries for neural network fundamentals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Linear Layers\n",
    "\n",
    "Linear layers (also called fully connected or dense layers) are fundamental components of neural networks. They perform the operation: **y = xW^T + b**\n",
    "\n",
    "**Components:**\n",
    "- **Weight Matrix (W)**: Learnable parameters that transform inputs\n",
    "- **Bias Vector (b)**: Learnable offset added to the transformation\n",
    "- **Input (x)**: Data to be transformed\n",
    "- **Output (y)**: Transformed result\n",
    "\n",
    "**In Transformers:**\n",
    "- Query, Key, Value projections in attention\n",
    "- Feed-forward network layers\n",
    "- Output projection layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Layer Analysis:\n",
      "Input features: 2\n",
      "Output features: 3\n",
      "Weight matrix shape: torch.Size([3, 2])\n",
      "Bias vector shape: torch.Size([3])\n",
      "Device: cuda:0\n",
      "\n",
      "Weight Matrix:\n",
      "tensor([[ 0.5406,  0.5869],\n",
      "        [-0.1657,  0.6496],\n",
      "        [-0.1549,  0.1427]], device='cuda:0')\n",
      "\n",
      "Bias Vector:\n",
      "tensor([-0.3443,  0.4153,  0.6233], device='cuda:0')\n",
      "\n",
      "Total parameters: 9\n",
      "   Weights: 6\n",
      "   Bias: 3\n"
     ]
    }
   ],
   "source": [
    "# Create a linear layer: 2 input features -> 3 output features\n",
    "linear = nn.Linear(in_features=2, out_features=3)\n",
    "\n",
    "# Move to AMD GPU\n",
    "linear = linear.to(device)\n",
    "\n",
    "print(\"Linear Layer Analysis:\")\n",
    "print(f\"Input features: {linear.in_features}\")\n",
    "print(f\"Output features: {linear.out_features}\")\n",
    "print(f\"Weight matrix shape: {linear.weight.shape}\")  # [out_features, in_features]\n",
    "print(f\"Bias vector shape: {linear.bias.shape}\")  # [out_features]\n",
    "print(f\"Device: {linear.weight.device}\")\n",
    "\n",
    "print(\"\\nWeight Matrix:\")\n",
    "print(linear.weight.data)\n",
    "print(\"\\nBias Vector:\")\n",
    "print(linear.bias.data)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in linear.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "print(f\"   Weights: {linear.weight.numel()}\")\n",
    "print(f\"   Bias: {linear.bias.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Tensor Operations and Matrix Multiplication\n",
    "\n",
    "Now let's explore how linear layers process multi-dimensional tensors and understand the mathematical operations involved.\n",
    "\n",
    "**Key Operations:**\n",
    "1. **Matrix Multiplication**: Core operation in linear layers\n",
    "2. **Tensor Broadcasting**: Automatic dimension expansion for operations\n",
    "3. **Batch Processing**: Handling multiple samples simultaneously\n",
    "4. **GPU Acceleration**: Parallel computation on AMD hardware\n",
    "\n",
    "**Mathematical Details:**\n",
    "- For input `x` with shape `[batch_size, ..., in_features]`\n",
    "- Weight `W` with shape `[out_features, in_features]`\n",
    "- Output `y = x @ W.T + b` with shape `[batch_size, ..., out_features]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Analysis:\n",
      "Input shape: torch.Size([2, 2, 2, 2])\n",
      "Input tensor:\n",
      "tensor([[[[ 0.1940,  2.1614],\n",
      "          [-0.1721,  0.8491]],\n",
      "\n",
      "         [[-1.9244,  0.6530],\n",
      "          [-0.6494, -0.8175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5280, -1.2753],\n",
      "          [-1.6621, -0.3033]],\n",
      "\n",
      "         [[-0.0926,  0.1992],\n",
      "          [-1.1204,  1.8577]]]], device='cuda:0')\n",
      "\n",
      "Output Analysis:\n",
      "Output shape: torch.Size([2, 2, 2, 3])\n",
      "Output tensor:\n",
      "tensor([[[[ 1.0291,  1.7871,  0.9017],\n",
      "          [ 0.0610,  0.9953,  0.7712]],\n",
      "\n",
      "         [[-1.0014,  1.1582,  1.0147],\n",
      "          [-1.1752, -0.0082,  0.6073]]],\n",
      "\n",
      "\n",
      "        [[[-0.8073, -0.5006,  0.3596],\n",
      "          [-1.4208,  0.4936,  0.8376]],\n",
      "\n",
      "         [[-0.2774,  0.5600,  0.6661],\n",
      "          [ 0.1403,  1.8075,  1.0620]]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Manual Computation Verification:\n",
      "After matrix multiplication shape: torch.Size([2, 2, 2, 3])\n",
      "After adding bias shape: torch.Size([2, 2, 2, 3])\n",
      "\n",
      "Verification:\n",
      "Manual computation matches linear layer: True\n",
      "\n",
      "Detailed Example (first element):\n",
      "Input [0,0,0,:]: tensor([0.1940, 2.1614], device='cuda:0')\n",
      "Weight matrix:\n",
      "Parameter containing:\n",
      "tensor([[ 0.5406,  0.5869],\n",
      "        [-0.1657,  0.6496],\n",
      "        [-0.1549,  0.1427]], device='cuda:0', requires_grad=True)\n",
      "Bias: Parameter containing:\n",
      "tensor([-0.3443,  0.4153,  0.6233], device='cuda:0', requires_grad=True)\n",
      "Manual result: tensor([1.0291, 1.7871, 0.9017], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Linear layer result: tensor([1.0291, 1.7871, 0.9017], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "# Create a multi-dimensional input tensor (simulating batch processing)\n",
    "# Shape: [batch_size=2, seq_length=2, height=2, in_features=2]\n",
    "x = torch.randn(2, 2, 2, 2, device=device)\n",
    "print(\"Input Tensor Analysis:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input tensor:\\n{x}\")\n",
    "\n",
    "# Apply linear transformation\n",
    "y = linear(x)\n",
    "print(\"\\nOutput Analysis:\")\n",
    "print(f\"Output shape: {y.shape}\")  # [2, 2, 2, 3]\n",
    "print(f\"Output tensor:\\n{y}\")\n",
    "\n",
    "print(\"\\nManual Computation Verification:\")\n",
    "# Manual matrix multiplication to understand the operation\n",
    "# Last dimension is transformed: [..., 2] -> [..., 3]\n",
    "\n",
    "# Method 1: Using torch.matmul\n",
    "y1 = torch.matmul(x, linear.weight.t())  # x @ W.T\n",
    "print(f\"After matrix multiplication shape: {y1.shape}\")\n",
    "\n",
    "# Add bias (broadcasting automatically handles the shape)\n",
    "y2 = y1 + linear.bias\n",
    "print(f\"After adding bias shape: {y2.shape}\")\n",
    "\n",
    "# Verify our manual computation matches PyTorch's linear layer\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Manual computation matches linear layer: {torch.allclose(y, y2, atol=1e-6)}\")\n",
    "\n",
    "# Show detailed computation for one element\n",
    "print(\"\\nDetailed Example (first element):\")\n",
    "print(f\"Input [0,0,0,:]: {x[0, 0, 0, :]}\")\n",
    "print(f\"Weight matrix:\\n{linear.weight}\")\n",
    "print(f\"Bias: {linear.bias}\")\n",
    "single_result = x[0, 0, 0, :] @ linear.weight.t() + linear.bias\n",
    "print(f\"Manual result: {single_result}\")\n",
    "print(f\"Linear layer result: {y[0, 0, 0, :]}\")\n",
    "print(f\"Match: {torch.allclose(single_result, y[0, 0, 0, :], atol=1e-6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Tensor Broadcasting\n",
    "\n",
    "Broadcasting is a powerful feature that allows operations between tensors of different shapes. It's crucial for efficient neural network computations.\n",
    "\n",
    "**Broadcasting Rules:**\n",
    "1. Align shapes from the rightmost dimension\n",
    "2. Dimensions of size 1 are stretched to match\n",
    "3. Missing dimensions are treated as size 1\n",
    "4. Incompatible shapes raise errors\n",
    "\n",
    "**Applications in Neural Networks:**\n",
    "- Adding bias vectors to matrix multiplication results\n",
    "- Element-wise operations with different shaped tensors\n",
    "- Scaling and normalization operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting Examples:\n",
      "Tensor a shape: torch.Size([2, 3, 4])\n",
      "Scalar b: 10.0\n",
      "a + b result shape: torch.Size([2, 3, 4])\n",
      "First element: a[0,0,0] = 0.139, result[0,0,0] = 10.139\n",
      "\n",
      "============================================================\n",
      "Tensor c shape: torch.Size([2, 3, 1])\n",
      "Tensor d shape: torch.Size([4])\n",
      "c + d result shape: torch.Size([2, 3, 4])\n",
      "\n",
      "Broadcasting visualization:\n",
      "c[0,0,0] = -0.519 gets added to each element of d\n",
      "d = tensor([-0.6974, -1.8688, -0.8832, -1.6627], device='cuda:0')\n",
      "Result c[0,0,:] + d = tensor([-1.2161, -2.3876, -1.4019, -2.1815], device='cuda:0')\n",
      "\n",
      "============================================================\n",
      "Attention scores shape: torch.Size([2, 4, 8, 8])\n",
      "Positional bias shape: torch.Size([1, 1, 8, 8])\n",
      "Result shape: torch.Size([2, 4, 8, 8])\n",
      "\n",
      "Broadcasting enables efficient operations without explicit tensor reshaping!\n",
      "\n",
      "Manual expansion vs Broadcasting:\n",
      "Manually expanded bias shape: torch.Size([2, 4, 8, 8])\n",
      "Memory usage - Original bias: 64 elements\n",
      "Memory usage - Expanded bias: 512 elements\n",
      "Broadcasting saves memory by not creating copies!\n"
     ]
    }
   ],
   "source": [
    "# Tensor Broadcasting Examples\n",
    "print(\"Broadcasting Examples:\")\n",
    "\n",
    "# Example 1: Scalar broadcasting\n",
    "a = torch.randn(2, 3, 4, device=device)\n",
    "b = 10.0  # scalar\n",
    "\n",
    "print(f\"Tensor a shape: {a.shape}\")\n",
    "print(f\"Scalar b: {b}\")\n",
    "\n",
    "# Broadcasting: scalar is automatically expanded\n",
    "result1 = a + b\n",
    "print(f\"a + b result shape: {result1.shape}\")\n",
    "print(f\"First element: a[0,0,0] = {a[0, 0, 0]:.3f}, result[0,0,0] = {result1[0, 0, 0]:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Example 2: Vector broadcasting\n",
    "c = torch.randn(2, 3, 1, device=device)  # Can broadcast to (2, 3, 4)\n",
    "d = torch.randn(4, device=device)  # Can broadcast to (2, 3, 4)\n",
    "\n",
    "print(f\"Tensor c shape: {c.shape}\")\n",
    "print(f\"Tensor d shape: {d.shape}\")\n",
    "\n",
    "result2 = c + d  # Broadcasting happens automatically\n",
    "print(f\"c + d result shape: {result2.shape}\")\n",
    "\n",
    "print(\"\\nBroadcasting visualization:\")\n",
    "print(f\"c[0,0,0] = {c[0, 0, 0].item():.3f} gets added to each element of d\")\n",
    "print(f\"d = {d}\")\n",
    "print(f\"Result c[0,0,:] + d = {result2[0, 0, :]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Example 3: Matrix broadcasting (common in attention mechanisms)\n",
    "# Simulate attention scores + positional bias\n",
    "attention_scores = torch.randn(2, 4, 8, 8, device=device)  # [batch, heads, seq, seq]\n",
    "positional_bias = torch.randn(1, 1, 8, 8, device=device)  # [1, 1, seq, seq]\n",
    "\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "print(f\"Positional bias shape: {positional_bias.shape}\")\n",
    "\n",
    "# Broadcasting automatically handles different batch/head dimensions\n",
    "attention_with_bias = attention_scores + positional_bias\n",
    "print(f\"Result shape: {attention_with_bias.shape}\")\n",
    "\n",
    "print(\"\\nBroadcasting enables efficient operations without explicit tensor reshaping!\")\n",
    "\n",
    "# Example 4: What happens without broadcasting (manual expansion)\n",
    "print(\"\\nManual expansion vs Broadcasting:\")\n",
    "positional_bias_expanded = positional_bias.expand_as(attention_scores)\n",
    "print(f\"Manually expanded bias shape: {positional_bias_expanded.shape}\")\n",
    "print(f\"Memory usage - Original bias: {positional_bias.numel()} elements\")\n",
    "print(f\"Memory usage - Expanded bias: {positional_bias_expanded.numel()} elements\")\n",
    "print(\"Broadcasting saves memory by not creating copies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Practical Applications in Transformers\n",
    "\n",
    "Let's see how these linear operations are used in transformer architectures with practical examples.\n",
    "\n",
    "**Transformer Applications:**\n",
    "1. **Attention Projections**: Q, K, V matrices\n",
    "2. **Feed-Forward Networks**: Two linear layers with activation\n",
    "3. **Output Projection**: Final linear layer for vocabulary prediction\n",
    "4. **Layer Normalization**: Learned scale and shift parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Component Simulation:\n",
      "Input embeddings shape: torch.Size([2, 8, 64])\n",
      "\n",
      "==================================================\n",
      "1. ATTENTION PROJECTIONS:\n",
      "Query (Q) shape: torch.Size([2, 8, 64])\n",
      "Key (K) shape: torch.Size([2, 8, 64])\n",
      "Value (V) shape: torch.Size([2, 8, 64])\n",
      "Attention scores shape: torch.Size([2, 8, 8])\n",
      "\n",
      "==================================================\n",
      "2. FEED-FORWARD NETWORK:\n",
      "Input to MLP: torch.Size([2, 8, 64])\n",
      "After first linear layer: torch.Size([2, 8, 256])\n",
      "After activation: torch.Size([2, 8, 256])\n",
      "After second linear layer: torch.Size([2, 8, 64])\n",
      "\n",
      "==================================================\n",
      "3. OUTPUT PROJECTION:\n",
      "Final logits shape: torch.Size([2, 8, 10000])\n",
      "\n",
      "All operations completed efficiently on AMD GPU!\n",
      "Q proj: 4,096 parameters\n",
      "K proj: 4,096 parameters\n",
      "V proj: 4,096 parameters\n",
      "FF layer 1: 16,640 parameters\n",
      "FF layer 2: 16,448 parameters\n",
      "Output proj: 650,000 parameters\n",
      "\n",
      "Total parameters: 695,376\n",
      "Memory usage: ~2.7 MB (float32)\n"
     ]
    }
   ],
   "source": [
    "# Simulate transformer components using linear layers\n",
    "\n",
    "print(\"Transformer Component Simulation:\")\n",
    "\n",
    "# Parameters for a mini transformer\n",
    "embed_dim = 64\n",
    "seq_length = 8\n",
    "batch_size = 2\n",
    "\n",
    "# Simulate input embeddings\n",
    "x = torch.randn(batch_size, seq_length, embed_dim, device=device)\n",
    "print(f\"Input embeddings shape: {x.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"1. ATTENTION PROJECTIONS:\")\n",
    "\n",
    "# Query, Key, Value projections (as used in self-attention)\n",
    "q_proj = nn.Linear(embed_dim, embed_dim, bias=False).to(device)\n",
    "k_proj = nn.Linear(embed_dim, embed_dim, bias=False).to(device)\n",
    "v_proj = nn.Linear(embed_dim, embed_dim, bias=False).to(device)\n",
    "\n",
    "# Apply projections\n",
    "Q = q_proj(x)  # Query\n",
    "K = k_proj(x)  # Key\n",
    "V = v_proj(x)  # Value\n",
    "\n",
    "print(f\"Query (Q) shape: {Q.shape}\")\n",
    "print(f\"Key (K) shape: {K.shape}\")\n",
    "print(f\"Value (V) shape: {V.shape}\")\n",
    "\n",
    "# Attention scores (simplified - no multi-head)\n",
    "attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2. FEED-FORWARD NETWORK:\")\n",
    "\n",
    "# Two-layer MLP as used in transformers\n",
    "mlp_dim = embed_dim * 4  # Common practice: 4x expansion\n",
    "ff_layer1 = nn.Linear(embed_dim, mlp_dim).to(device)\n",
    "ff_layer2 = nn.Linear(mlp_dim, embed_dim).to(device)\n",
    "\n",
    "# Forward pass through MLP\n",
    "ff_out1 = ff_layer1(x)\n",
    "ff_out1_activated = F.relu(ff_out1)  # ReLU activation\n",
    "ff_out2 = ff_layer2(ff_out1_activated)\n",
    "\n",
    "print(f\"Input to MLP: {x.shape}\")\n",
    "print(f\"After first linear layer: {ff_out1.shape}\")\n",
    "print(f\"After activation: {ff_out1_activated.shape}\")\n",
    "print(f\"After second linear layer: {ff_out2.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3. OUTPUT PROJECTION:\")\n",
    "\n",
    "# Final projection to vocabulary (e.g., for language modeling)\n",
    "vocab_size = 10000\n",
    "output_proj = nn.Linear(embed_dim, vocab_size).to(device)\n",
    "\n",
    "# Apply output projection\n",
    "logits = output_proj(x)\n",
    "print(f\"Final logits shape: {logits.shape}\")  # [batch, seq_length, vocab_size]\n",
    "\n",
    "print(\"\\nAll operations completed efficiently on AMD GPU!\")\n",
    "\n",
    "# Parameter counting\n",
    "total_params = 0\n",
    "for name, layer in [\n",
    "    (\"Q proj\", q_proj),\n",
    "    (\"K proj\", k_proj),\n",
    "    (\"V proj\", v_proj),\n",
    "    (\"FF layer 1\", ff_layer1),\n",
    "    (\"FF layer 2\", ff_layer2),\n",
    "    (\"Output proj\", output_proj),\n",
    "]:\n",
    "    params = sum(p.numel() for p in layer.parameters())\n",
    "    total_params += params\n",
    "    print(f\"{name}: {params:,} parameters\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Memory usage: ~{total_params * 4 / 1024**2:.1f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "### Technical Concepts Learned\n",
    "- **Linear Transformations**: The mathematical foundation y = xW^T + b\n",
    "- **Parameter Management**: Weight initialization and device placement\n",
    "- **Batch Processing**: Handling multiple samples simultaneously\n",
    "- **Memory Efficiency**: Broadcasting vs explicit tensor expansion\n",
    "- **Transformer Architecture**: How linear layers build complex models\n",
    "\n",
    "### Experiment Further\n",
    "Try these modifications to deepen your understanding:\n",
    "- Change embedding dimensions and observe parameter scaling\n",
    "- Experiment with different tensor shapes and broadcasting\n",
    "- Compare CPU vs GPU computation times\n",
    "- Implement custom linear layers from scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
