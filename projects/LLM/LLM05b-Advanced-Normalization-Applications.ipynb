{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5b: Advanced Normalization Applications in Transformers\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "Welcome to an in-depth exploration of advanced normalization techniques used in modern transformer architectures! This lab builds upon basic normalization concepts to demonstrate sophisticated applications in large language models and neural networks.\n",
    "\n",
    "**Lab Goal**: Master advanced normalization techniques including RMSNorm, Pre-Norm vs Post-Norm architectures, and positional encoding normalization.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Implement RMSNorm**: Understand and code Root Mean Square Layer Normalization\n",
    "2. **Compare Normalization Strategies**: Analyze Pre-Norm vs Post-Norm transformer architectures\n",
    "3. **Apply Positional Encoding**: Implement and normalize sinusoidal positional embeddings\n",
    "4. **Analyze Performance**: Compare different normalization techniques quantitatively\n",
    "5. **Connect to LLMs**: Understand how these techniques are used in models like LLaMA and GPT\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.1+rocm7.10.0\n",
      "GPU: Radeon 8060S Graphics\n",
      "GPU Memory: 103.1 GB\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RMSNorm (Root Mean Square Layer Normalization)\n",
    "\n",
    "RMSNorm is a simplified and more efficient alternative to LayerNorm, used in modern models like LLaMA. Instead of centering the data by subtracting the mean, RMSNorm only normalizes by the root mean square.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "- **Standard LayerNorm**: `y = (x - μ) / σ * γ + β`\n",
    "- **RMSNorm**: `y = x / RMS(x) * γ` where `RMS(x) = √(mean(x²) + ε)`\n",
    "\n",
    "**Key Advantages:**\n",
    "- **Computational Efficiency**: Fewer operations (no mean computation/subtraction)\n",
    "- **Memory Efficiency**: Reduced intermediate tensor storage\n",
    "- **Numerical Stability**: Often more stable than LayerNorm\n",
    "- **Performance**: Faster on GPU due to simpler operations\n",
    "\n",
    "**LLaMA Connection**: Meta's LLaMA models use RMSNorm in every transformer layer for better efficiency and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RMSNorm Implementation\n",
      "Input shape: torch.Size([2, 4, 8])\n",
      "Input device: cuda:0\n",
      "\n",
      " RMSNorm initialized on cuda\n",
      "Parameters: 8\n",
      "\n",
      "Output shape: torch.Size([2, 4, 8])\n",
      "Output device: cuda:0\n",
      "\n",
      " Normalization Analysis:\n",
      "Input mean: 0.0328\n",
      "Input std: 1.0286\n",
      "Output mean: 0.0356\n",
      "Output std: 0.9911\n",
      "\n",
      "RMS Analysis:\n",
      "Input RMS: 1.0312\n",
      "Output RMS: 1.0000 (should be ≈ weight scale)\n",
      "Weight values: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Implement RMSNorm from Scratch\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        RMSNorm layer as used in LLaMA and other modern models.\n",
    "\n",
    "        Args:\n",
    "            dim: The dimension to normalize (usually the last dimension)\n",
    "            eps: Small epsilon to prevent division by zero\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"Compute the RMS normalization\"\"\"\n",
    "        # Compute RMS: sqrt(mean(x^2) + eps)\n",
    "        rms = x.pow(2).mean(-1, keepdim=True).add(self.eps).sqrt()\n",
    "        return x / rms\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with learnable scaling\"\"\"\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "# Create test data\n",
    "batch_size, seq_len, hidden_dim = 2, 4, 8\n",
    "input_tensor = torch.randn(batch_size, seq_len, hidden_dim).to(device)\n",
    "\n",
    "print(\"Testing RMSNorm Implementation\")\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Input device: {input_tensor.device}\")\n",
    "\n",
    "# Initialize RMSNorm\n",
    "rms_norm = RMSNorm(hidden_dim).to(device)\n",
    "print(f\"\\n RMSNorm initialized on {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in rms_norm.parameters())}\")\n",
    "\n",
    "# Apply RMSNorm\n",
    "normalized_output = rms_norm(input_tensor)\n",
    "print(f\"\\nOutput shape: {normalized_output.shape}\")\n",
    "print(f\"Output device: {normalized_output.device}\")\n",
    "\n",
    "# Analyze normalization effect\n",
    "print(\"\\n Normalization Analysis:\")\n",
    "print(f\"Input mean: {input_tensor.mean(-1).mean():.4f}\")\n",
    "print(f\"Input std: {input_tensor.std(-1).mean():.4f}\")\n",
    "print(f\"Output mean: {normalized_output.mean(-1).mean():.4f}\")\n",
    "print(f\"Output std: {normalized_output.std(-1).mean():.4f}\")\n",
    "\n",
    "# Show RMS values\n",
    "input_rms = input_tensor.pow(2).mean(-1).sqrt()\n",
    "output_rms = normalized_output.pow(2).mean(-1).sqrt()\n",
    "print(\"\\nRMS Analysis:\")\n",
    "print(f\"Input RMS: {input_rms.mean():.4f}\")\n",
    "print(f\"Output RMS: {output_rms.mean():.4f} (should be ≈ weight scale)\")\n",
    "print(f\"Weight values: {rms_norm.weight.data[:5]}\")  # Show first 5 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing RMSNorm vs LayerNorm\n",
      "\n",
      " Performance Comparison:\n",
      "==================================================\n",
      "RMSNorm time (100 iterations): 0.0023s\n",
      "LayerNorm time (100 iterations): 0.0008s\n",
      "Speedup: 0.34x\n",
      "\n",
      " Statistical Comparison:\n",
      "==================================================\n",
      "Original input:\n",
      "  Mean: tensor([[ 0.0007, -0.2742,  0.5283,  0.6174],\n",
      "        [-0.1671, -0.3646, -0.2268,  0.1488]], device='cuda:0')\n",
      "  Std:  tensor([[1.4991, 0.9872, 0.9361, 0.6845],\n",
      "        [0.9583, 1.2751, 0.7560, 1.1328]], device='cuda:0')\n",
      "\n",
      "RMSNorm output:\n",
      "  Mean: tensor([[ 4.9339e-04, -2.8460e-01,  5.1661e-01,  6.9412e-01],\n",
      "        [-1.8321e-01, -2.9233e-01, -3.0544e-01,  1.3909e-01]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "  Std:  tensor([[1.0690, 1.0248, 0.9153, 0.7696],\n",
      "        [1.0509, 1.0223, 1.0180, 1.0587]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "LayerNorm output:\n",
      "  Mean: tensor([[ 1.4901e-08,  7.4506e-09,  2.9802e-08, -2.9802e-08],\n",
      "        [ 2.6077e-08, -1.4901e-08, -1.4901e-08, -4.4703e-08]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "  Std:  tensor([[1.0690, 1.0690, 1.0690, 1.0690],\n",
      "        [1.0690, 1.0690, 1.0690, 1.0690]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      " Gradient Flow Analysis:\n",
      "RMSNorm weight gradient norm: 0.9325\n",
      "LayerNorm weight gradient norm: 0.9060\n",
      "LayerNorm bias gradient norm: 0.2748\n",
      "\n",
      " Key Insights:\n",
      "- RMSNorm is typically 0.3x faster than LayerNorm\n",
      "- RMSNorm doesn't center data (non-zero mean)\n",
      "- RMSNorm uses fewer parameters (no bias term)\n",
      "- Both provide similar gradient flow characteristics\n"
     ]
    }
   ],
   "source": [
    "# Compare RMSNorm vs LayerNorm Performance and Behavior\n",
    "print(\"Comparing RMSNorm vs LayerNorm\")\n",
    "\n",
    "# Create standard LayerNorm for comparison\n",
    "layer_norm = nn.LayerNorm(hidden_dim).to(device)\n",
    "\n",
    "# Test with the same input\n",
    "ln_output = layer_norm(input_tensor)\n",
    "\n",
    "print(\"\\n Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Time comparison (simplified - for demonstration)\n",
    "import time\n",
    "\n",
    "# RMSNorm timing\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    _ = rms_norm(input_tensor)\n",
    "rms_time = time.time() - start_time\n",
    "\n",
    "# LayerNorm timing\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    _ = layer_norm(input_tensor)\n",
    "ln_time = time.time() - start_time\n",
    "\n",
    "print(f\"RMSNorm time (100 iterations): {rms_time:.4f}s\")\n",
    "print(f\"LayerNorm time (100 iterations): {ln_time:.4f}s\")\n",
    "print(f\"Speedup: {ln_time / rms_time:.2f}x\")\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"\\n Statistical Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Original input:\")\n",
    "print(f\"  Mean: {input_tensor.mean(-1)[:2]}\")  # First 2 samples\n",
    "print(f\"  Std:  {input_tensor.std(-1)[:2]}\")\n",
    "\n",
    "print(\"\\nRMSNorm output:\")\n",
    "print(f\"  Mean: {normalized_output.mean(-1)[:2]}\")\n",
    "print(f\"  Std:  {normalized_output.std(-1)[:2]}\")\n",
    "\n",
    "print(\"\\nLayerNorm output:\")\n",
    "print(f\"  Mean: {ln_output.mean(-1)[:2]}\")\n",
    "print(f\"  Std:  {ln_output.std(-1)[:2]}\")\n",
    "\n",
    "# Gradient flow comparison\n",
    "print(\"\\n Gradient Flow Analysis:\")\n",
    "# Create a simple loss and backpropagate\n",
    "target = torch.randn_like(input_tensor).to(device)\n",
    "\n",
    "# RMSNorm gradients\n",
    "rms_loss = F.mse_loss(normalized_output, target)\n",
    "rms_loss.backward(retain_graph=True)\n",
    "rms_grad_norm = rms_norm.weight.grad.norm().item()\n",
    "\n",
    "# LayerNorm gradients\n",
    "ln_loss = F.mse_loss(ln_output, target)\n",
    "ln_loss.backward()\n",
    "ln_weight_grad_norm = layer_norm.weight.grad.norm().item()\n",
    "ln_bias_grad_norm = layer_norm.bias.grad.norm().item()\n",
    "\n",
    "print(f\"RMSNorm weight gradient norm: {rms_grad_norm:.4f}\")\n",
    "print(f\"LayerNorm weight gradient norm: {ln_weight_grad_norm:.4f}\")\n",
    "print(f\"LayerNorm bias gradient norm: {ln_bias_grad_norm:.4f}\")\n",
    "\n",
    "print(\"\\n Key Insights:\")\n",
    "print(f\"- RMSNorm is typically {ln_time / rms_time:.1f}x faster than LayerNorm\")\n",
    "print(\"- RMSNorm doesn't center data (non-zero mean)\")\n",
    "print(\"- RMSNorm uses fewer parameters (no bias term)\")\n",
    "print(\"- Both provide similar gradient flow characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Pre-Norm vs Post-Norm Transformer Architectures\n",
    "\n",
    "The placement of normalization layers in transformer architectures significantly impacts training dynamics and model performance. Let's explore both approaches and their implications.\n",
    "\n",
    "**Architecture Comparison:**\n",
    "\n",
    "**Post-Norm (Original Transformer):**\n",
    "```\n",
    "x → MultiHeadAttention → Add & Norm → FFN → Add & Norm → output\n",
    "```\n",
    "\n",
    "**Pre-Norm (Modern Approach):**\n",
    "```\n",
    "x → Norm → MultiHeadAttention → Add → Norm → FFN → Add → output\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "**Post-Norm Characteristics:**\n",
    "- **Residual Path**: Clean residual connections from input to output\n",
    "- **Gradient Flow**: Can suffer from gradient vanishing in deep networks\n",
    "- **Training Stability**: May require careful initialization and learning rates\n",
    "- **Performance**: Often achieves better final performance when trained successfully\n",
    "\n",
    "**Pre-Norm Characteristics:**\n",
    "- **Training Stability**: More stable training, especially for deep networks\n",
    "- **Gradient Flow**: Better gradient flow through normalized paths\n",
    "- **Warmup Requirements**: Often requires less careful warmup strategies\n",
    "- **Scalability**: Easier to scale to very deep architectures\n",
    "\n",
    "**Real-World Usage:**\n",
    "- **GPT Models**: Use Pre-Norm for better training stability\n",
    "- **T5**: Uses Pre-Norm architecture\n",
    "- **BERT**: Uses Post-Norm (original transformer style)\n",
    "- **Modern LLMs**: Mostly adopt Pre-Norm for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Block Comparison\n",
      "Input shape: torch.Size([4, 32, 128])\n",
      "Hidden dimension: 128\n",
      "\n",
      "Testing Forward Passes:\n",
      "Post-Norm output shape: torch.Size([4, 32, 128])\n",
      "Pre-Norm output shape: torch.Size([4, 32, 128])\n",
      "\n",
      " Output Statistics:\n",
      "Post-Norm - Mean: 0.0059, Std: 1.0000\n",
      "Pre-Norm - Mean: 0.0116, Std: 1.0248\n",
      "\n",
      " Parameter Comparison:\n",
      "Post-Norm parameters: 197,504\n",
      "Pre-Norm parameters: 197,504\n",
      "Parameter difference: 0\n"
     ]
    }
   ],
   "source": [
    "# Implement Pre-Norm and Post-Norm Transformer Blocks\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simplified attention mechanism for demonstration\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.out = nn.Linear(dim, dim, bias=False)\n",
    "        self.scale = dim**-0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, C).permute(2, 0, 1, 3)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Simplified attention (no multi-head for clarity)\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        out = att @ v\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "class PostNormBlock(nn.Module):\n",
    "    \"\"\"Post-Norm Transformer Block (Original Transformer style)\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attention = SimpleAttention(dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim))\n",
    "        self.ln1 = RMSNorm(dim)\n",
    "        self.ln2 = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Post-Norm: x → Attention → Add & Norm → FFN → Add & Norm\n",
    "        x = self.ln1(x + self.attention(x))\n",
    "        x = self.ln2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PreNormBlock(nn.Module):\n",
    "    \"\"\"Pre-Norm Transformer Block (Modern style)\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attention = SimpleAttention(dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim))\n",
    "        self.ln1 = RMSNorm(dim)\n",
    "        self.ln2 = RMSNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-Norm: x → Norm → Attention → Add → Norm → FFN → Add\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create test models\n",
    "dim = 128\n",
    "seq_len = 32\n",
    "batch_size = 4\n",
    "\n",
    "test_input = torch.randn(batch_size, seq_len, dim).to(device)\n",
    "\n",
    "post_norm_block = PostNormBlock(dim).to(device)\n",
    "pre_norm_block = PreNormBlock(dim).to(device)\n",
    "\n",
    "print(\"Transformer Block Comparison\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Hidden dimension: {dim}\")\n",
    "\n",
    "# Test forward passes\n",
    "print(\"\\nTesting Forward Passes:\")\n",
    "post_norm_output = post_norm_block(test_input)\n",
    "pre_norm_output = pre_norm_block(test_input)\n",
    "\n",
    "print(f\"Post-Norm output shape: {post_norm_output.shape}\")\n",
    "print(f\"Pre-Norm output shape: {pre_norm_output.shape}\")\n",
    "\n",
    "# Analyze output statistics\n",
    "print(\"\\n Output Statistics:\")\n",
    "print(f\"Post-Norm - Mean: {post_norm_output.mean():.4f}, Std: {post_norm_output.std():.4f}\")\n",
    "print(f\"Pre-Norm - Mean: {pre_norm_output.mean():.4f}, Std: {pre_norm_output.std():.4f}\")\n",
    "\n",
    "# Parameter count comparison\n",
    "post_norm_params = sum(p.numel() for p in post_norm_block.parameters())\n",
    "pre_norm_params = sum(p.numel() for p in pre_norm_block.parameters())\n",
    "\n",
    "print(\"\\n Parameter Comparison:\")\n",
    "print(f\"Post-Norm parameters: {post_norm_params:,}\")\n",
    "print(f\"Pre-Norm parameters: {pre_norm_params:,}\")\n",
    "print(f\"Parameter difference: {abs(post_norm_params - pre_norm_params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gradient Flow Analysis\n",
      "\n",
      " Loss Comparison:\n",
      "Post-Norm loss: 2.0103\n",
      "Pre-Norm loss: 2.0617\n",
      "\n",
      " Gradient Norms for Normalization Layers:\n",
      "==================================================\n",
      "Post-Norm:\n",
      "  LayerNorm 1 gradient norm: 0.0144\n",
      "  LayerNorm 2 gradient norm: 0.1798\n",
      "Pre-Norm:\n",
      "  LayerNorm 1 gradient norm: 0.0023\n",
      "  LayerNorm 2 gradient norm: 0.0097\n",
      "\n",
      " Detailed Gradient Analysis:\n",
      "============================================================\n",
      "Parameter                      Post-Norm    Pre-Norm    \n",
      "============================================================\n",
      "ln1.weight                     0.0144       0.0023      \n",
      "ln2.weight                     0.1798       0.0097      \n",
      "attention.qkv.weight           0.0231       0.0361      \n",
      "attention.out.weight           0.0207       0.0324      \n",
      "\n",
      " Training Insights:\n",
      "- Pre-Norm typically shows more stable gradient flow\n",
      "- Post-Norm may have larger gradient variations\n",
      "- Pre-Norm normalization layers often have smaller gradients\n",
      "- Both architectures can be trained successfully with proper setup\n"
     ]
    }
   ],
   "source": [
    "# Analyze Gradient Flow in Pre-Norm vs Post-Norm\n",
    "print(\" Gradient Flow Analysis\")\n",
    "\n",
    "# Create a simple loss for both models\n",
    "target = torch.randn_like(test_input).to(device)\n",
    "\n",
    "# Post-Norm gradient analysis\n",
    "post_norm_block.zero_grad()\n",
    "post_loss = F.mse_loss(post_norm_output, target)\n",
    "post_loss.backward(retain_graph=True)\n",
    "\n",
    "# Pre-Norm gradient analysis\n",
    "pre_norm_block.zero_grad()\n",
    "pre_loss = F.mse_loss(pre_norm_output, target)\n",
    "pre_loss.backward()\n",
    "\n",
    "print(\"\\n Loss Comparison:\")\n",
    "print(f\"Post-Norm loss: {post_loss.item():.4f}\")\n",
    "print(f\"Pre-Norm loss: {pre_loss.item():.4f}\")\n",
    "\n",
    "# Analyze gradient norms for normalization layers\n",
    "post_ln1_grad = post_norm_block.ln1.weight.grad.norm().item()\n",
    "post_ln2_grad = post_norm_block.ln2.weight.grad.norm().item()\n",
    "pre_ln1_grad = pre_norm_block.ln1.weight.grad.norm().item()\n",
    "pre_ln2_grad = pre_norm_block.ln2.weight.grad.norm().item()\n",
    "\n",
    "print(\"\\n Gradient Norms for Normalization Layers:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Post-Norm:\")\n",
    "print(f\"  LayerNorm 1 gradient norm: {post_ln1_grad:.4f}\")\n",
    "print(f\"  LayerNorm 2 gradient norm: {post_ln2_grad:.4f}\")\n",
    "print(\"Pre-Norm:\")\n",
    "print(f\"  LayerNorm 1 gradient norm: {pre_ln1_grad:.4f}\")\n",
    "print(f\"  LayerNorm 2 gradient norm: {pre_ln2_grad:.4f}\")\n",
    "\n",
    "\n",
    "# Create a visualization of gradient magnitudes\n",
    "def get_gradient_stats(model, name):\n",
    "    grad_norms = []\n",
    "    param_names = []\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "            param_names.append(f\"{name}_{param_name}\")\n",
    "    return grad_norms, param_names\n",
    "\n",
    "\n",
    "post_grads, post_names = get_gradient_stats(post_norm_block, \"PostNorm\")\n",
    "pre_grads, pre_names = get_gradient_stats(pre_norm_block, \"PreNorm\")\n",
    "\n",
    "print(\"\\n Detailed Gradient Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Parameter':<30} {'Post-Norm':<12} {'Pre-Norm':<12}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare common parameters\n",
    "common_params = [\"ln1.weight\", \"ln2.weight\", \"attention.qkv.weight\", \"attention.out.weight\"]\n",
    "for param in common_params:\n",
    "    post_grad = next((g for g, n in zip(post_grads, post_names) if param in n), 0)\n",
    "    pre_grad = next((g for g, n in zip(pre_grads, pre_names) if param in n), 0)\n",
    "    print(f\"{param:<30} {post_grad:<12.4f} {pre_grad:<12.4f}\")\n",
    "\n",
    "print(\"\\n Training Insights:\")\n",
    "print(\"- Pre-Norm typically shows more stable gradient flow\")\n",
    "print(\"- Post-Norm may have larger gradient variations\")\n",
    "print(\"- Pre-Norm normalization layers often have smaller gradients\")\n",
    "print(\"- Both architectures can be trained successfully with proper setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Positional Encoding with Normalization\n",
    "\n",
    "Positional encodings are crucial for transformers to understand sequence order. We'll implement sinusoidal positional encodings and explore how normalization affects their integration with input embeddings.\n",
    "\n",
    "**Sinusoidal Positional Encoding Formula:**\n",
    "- `PE(pos, 2i) = sin(pos / 10000^(2i/d))`\n",
    "- `PE(pos, 2i+1) = cos(pos / 10000^(2i/d))`\n",
    "\n",
    "Where:\n",
    "- `pos` is the position in the sequence\n",
    "- `i` is the dimension index\n",
    "- `d` is the model dimension\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Frequency Modulation**: Different dimensions use different frequencies\n",
    "- **Relative Position**: Model can learn relative positions through dot products\n",
    "- **Extrapolation**: Can potentially handle longer sequences than seen during training\n",
    "- **Normalization Impact**: How normalization affects the integration of positional information\n",
    "\n",
    "**Advanced Applications:**\n",
    "- **RoPE (Rotary Position Embedding)**: Used in modern models like LLaMA\n",
    "- **ALiBi (Attention with Linear Biases)**: Alternative position encoding method\n",
    "- **Learned Positional Embeddings**: Trainable position representations\n",
    "- **Relative Position Encoding**: Direct relative position modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Advanced Positional Encoding Implementation\n",
      "Input embeddings shape: torch.Size([2, 20, 64])\n",
      "Encoded embeddings shape: torch.Size([2, 20, 64])\n",
      "Positional encoding matrix shape: torch.Size([20, 64])\n",
      "\n",
      " Positional Encoding Analysis:\n",
      "PE range: [-1.000, 1.000]\n",
      "PE mean: 0.438697\n",
      "PE std: 0.554784\n",
      "\n",
      " Frequency Patterns (first 8 dimensions):\n",
      "Dim  0- 1: frequency = 1.000000\n",
      "Dim  2- 3: frequency = 0.749894\n",
      "Dim  4- 5: frequency = 0.562341\n",
      "Dim  6- 7: frequency = 0.421697\n",
      "\n",
      " Embedding Comparison:\n",
      "Original embedding norm: 8.1270\n",
      "Encoded embedding norm: 10.5577\n",
      "Positional encoding contribution: 5.6569\n"
     ]
    }
   ],
   "source": [
    "# Implement Advanced Positional Encoding with Normalization\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding as used in the original Transformer\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Create frequency dividers\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "\n",
    "        # Apply sine and cosine\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape: [max_len, 1, dim]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings\"\"\"\n",
    "        # x shape: [batch_size, seq_len, dim]\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:seq_len, :, :].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Test positional encoding\n",
    "print(\" Advanced Positional Encoding Implementation\")\n",
    "\n",
    "dim = 64\n",
    "max_len = 100\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "\n",
    "# Create test embeddings\n",
    "embeddings = torch.randn(batch_size, seq_len, dim).to(device)\n",
    "print(f\"Input embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Initialize positional encoding\n",
    "pos_encoder = SinusoidalPositionalEncoding(dim, max_len).to(device)\n",
    "\n",
    "# Apply positional encoding\n",
    "encoded_embeddings = pos_encoder(embeddings)\n",
    "print(f\"Encoded embeddings shape: {encoded_embeddings.shape}\")\n",
    "\n",
    "# Visualize positional encoding patterns\n",
    "pe_matrix = pos_encoder.pe[:seq_len, 0, :].cpu()  # Get PE for visualization\n",
    "print(f\"Positional encoding matrix shape: {pe_matrix.shape}\")\n",
    "\n",
    "# Analyze positional encoding properties\n",
    "print(\"\\n Positional Encoding Analysis:\")\n",
    "print(f\"PE range: [{pe_matrix.min():.3f}, {pe_matrix.max():.3f}]\")\n",
    "print(f\"PE mean: {pe_matrix.mean():.6f}\")\n",
    "print(f\"PE std: {pe_matrix.std():.6f}\")\n",
    "\n",
    "# Show frequency patterns for first few dimensions\n",
    "print(\"\\n Frequency Patterns (first 8 dimensions):\")\n",
    "for i in range(0, min(8, dim), 2):\n",
    "    freq = 1 / (10000 ** (i / dim))\n",
    "    print(f\"Dim {i:2d}-{i + 1:2d}: frequency = {freq:.6f}\")\n",
    "\n",
    "# Compare embeddings before and after positional encoding\n",
    "print(\"\\n Embedding Comparison:\")\n",
    "print(f\"Original embedding norm: {embeddings.norm(dim=-1).mean():.4f}\")\n",
    "print(f\"Encoded embedding norm: {encoded_embeddings.norm(dim=-1).mean():.4f}\")\n",
    "print(f\"Positional encoding contribution: {pos_encoder.pe[:seq_len, 0, :].norm(dim=-1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Normalization Impact on Positional Encoding\n",
      "\n",
      " Normalization Impact Comparison:\n",
      "======================================================================\n",
      "Scenario                  Mean       Std        L2 Norm   \n",
      "======================================================================\n",
      "No Normalization          0.0296     1.0144     8.1270    \n",
      "RMSNorm After PE          0.3252     0.9481     8.0000    \n",
      "RMSNorm Before PE         0.4410     1.2132     10.3244   \n",
      "LayerNorm After PE        -0.0000    1.0079     8.0000    \n",
      "\n",
      " Positional Information Analysis:\n",
      "==================================================\n",
      "No Normalization         : Position correlation = 1.0000\n",
      "RMSNorm After PE         : Position correlation = 0.8034\n",
      "LayerNorm After PE       : Position correlation = 0.4450\n",
      "\n",
      " Key Insights:\n",
      "- Normalization AFTER PE preserves relative positional information\n",
      "- Normalization BEFORE PE can destroy positional patterns\n",
      "- RMSNorm tends to preserve more positional information than LayerNorm\n",
      "- The interaction between embeddings and PE is crucial for model performance\n",
      "\n",
      " First 5 Positions Comparison:\n",
      "No Normalization: tensor([-1.0904, -0.0875, -0.5452, -0.4202,  0.5460], device='cuda:0')\n",
      "RMSNorm After PE: tensor([-0.2860,  1.9722,  0.4043, -0.1378, -0.4141], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Relative ordering preserved: False\n"
     ]
    }
   ],
   "source": [
    "# Analyze Impact of Normalization on Positional Encoding\n",
    "print(\" Normalization Impact on Positional Encoding\")\n",
    "\n",
    "# Create different normalization scenarios\n",
    "scenarios = {\n",
    "    \"No Normalization\": lambda x: x,\n",
    "    \"RMSNorm After PE\": lambda x: RMSNorm(dim).to(device)(pos_encoder(x)),\n",
    "    \"RMSNorm Before PE\": lambda x: pos_encoder(RMSNorm(dim).to(device)(x)),\n",
    "    \"LayerNorm After PE\": lambda x: nn.LayerNorm(dim).to(device)(pos_encoder(x)),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, transform in scenarios.items():\n",
    "    # Apply transformation\n",
    "    if \"RMSNorm\" in name or \"LayerNorm\" in name:\n",
    "        # Create fresh embeddings for each test\n",
    "        test_embeddings = torch.randn(batch_size, seq_len, dim).to(device)\n",
    "        output = transform(test_embeddings)\n",
    "    else:\n",
    "        output = transform(embeddings)\n",
    "\n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"output\": output,\n",
    "        \"mean\": output.mean(-1).mean().item(),\n",
    "        \"std\": output.std(-1).mean().item(),\n",
    "        \"norm\": output.norm(dim=-1).mean().item(),\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n Normalization Impact Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Scenario':<25} {'Mean':<10} {'Std':<10} {'L2 Norm':<10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, stats in results.items():\n",
    "    print(f\"{name:<25} {stats['mean']:<10.4f} {stats['std']:<10.4f} {stats['norm']:<10.4f}\")\n",
    "\n",
    "# Analyze positional information preservation\n",
    "print(\"\\n Positional Information Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create position-specific patterns to test preservation\n",
    "position_test = torch.zeros(1, seq_len, dim).to(device)\n",
    "position_test[0, :, 0] = torch.arange(seq_len, dtype=torch.float)  # Position signal in first dim\n",
    "\n",
    "for name, transform in scenarios.items():\n",
    "    if name == \"No Normalization\":\n",
    "        test_output = transform(position_test)\n",
    "    else:\n",
    "        # Need to handle normalization properly\n",
    "        if \"Before PE\" in name:\n",
    "            # This would destroy positional info, so skip\n",
    "            continue\n",
    "        else:\n",
    "            test_output = transform(position_test)\n",
    "\n",
    "    # Check if position signal is preserved\n",
    "    position_signal = test_output[0, :, 0]\n",
    "    correlation = torch.corrcoef(torch.stack([torch.arange(seq_len, dtype=torch.float), position_signal.cpu()]))[0, 1]\n",
    "\n",
    "    print(f\"{name:<25}: Position correlation = {correlation:.4f}\")\n",
    "\n",
    "# Show actual values for first few positions\n",
    "print(\"\\n First 5 Positions Comparison:\")\n",
    "no_norm_vals = results[\"No Normalization\"][\"output\"][0, :5, 0]\n",
    "after_norm_vals = results[\"RMSNorm After PE\"][\"output\"][0, :5, 0]\n",
    "\n",
    "print(f\"No Normalization: {no_norm_vals}\")\n",
    "print(f\"RMSNorm After PE: {after_norm_vals}\")\n",
    "print(f\"Relative ordering preserved: {torch.argsort(no_norm_vals).equal(torch.argsort(after_norm_vals))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "### Technical Concepts Learned\n",
    "- **RMSNorm Implementation**: Building RMSNorm from scratch and comparing its efficiency with LayerNorm\n",
    "- **Pre-Norm vs Post-Norm**: Understanding how normalization placement affects gradient flow and training stability\n",
    "- **Sinusoidal Positional Encoding**: Implementing frequency-based position embeddings using sin/cos functions\n",
    "- **Normalization Order Effects**: How normalization before vs after positional encoding impacts position information preservation\n",
    "- **Gradient Flow Analysis**: Comparing gradient magnitudes through different transformer block architectures\n",
    "\n",
    "### Experiment Further\n",
    "- Implement RoPE (Rotary Position Embedding) and compare with sinusoidal PE\n",
    "- Stack multiple Pre-Norm vs Post-Norm blocks and compare training stability at depth\n",
    "- Measure actual GPU memory and compute time for RMSNorm vs LayerNorm at scale\n",
    "- Test normalization impact on longer sequences (extrapolation beyond training length)\n",
    "- Implement DeepNorm for training very deep transformer models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
