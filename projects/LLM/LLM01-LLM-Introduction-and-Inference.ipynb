{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Hello World with LLaMA - Introduction to Large Language Models\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "This lab introduces you to working with Large Language Models (LLMs) using the LLaMA architecture. You'll learn how to load a pre-trained model, tokenize text, and generate responses.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand how to load and use pre-trained LLM models\n",
    "- Learn about tokenization and text preprocessing\n",
    "- Experience text generation with transformer models\n",
    "- Understand the basic workflow of LLM inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "We'll configure our environment for LLM inference using AMD GPU backend for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD GPU environment initialized successfully\n",
      "Using device: cuda\n",
      "PyTorch version: 2.6.0+gitdbfe118\n",
      "GPU: AMD Radeon Graphics\n",
      "GPU Memory: 68.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for LLM inference\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer, LlamaForCausalLM\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Loading and Configuration\n",
    "\n",
    "Now we'll load a pre-trained LLaMA model. For this lab, we'll use a publicly available model that demonstrates the core concepts.\n",
    "\n",
    "**Model Loading Process:**\n",
    "1. **Model Architecture**: Load the LLaMA model with causal language modeling head\n",
    "2. **Tokenizer**: Load the corresponding tokenizer for text preprocessing\n",
    "3. **GPU Transfer**: Move model to AMD GPU for efficient inference\n",
    "\n",
    "**Note**: In a production environment, you would use the official LLaMA models. For this lab, we'll use a compatible model that demonstrates the same concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f1740088264d8eab2c2e3829f12de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba3bafe314848d1b5a1c972156fae24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e604cf28f8514b1189975b6b219ffa05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda\n",
      "Model parameters: 1,100,048,384\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA model (using a smaller model for demonstration)\n",
    "# Note: Replace with actual model path or use a publicly available model\n",
    "try:\n",
    "    # For this demo, we'll use a smaller compatible model\n",
    "    # In practice, you would use the official LLaMA model path\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Using a smaller Llama model for demonstration\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,  # Use half precision for AMD GPU efficiency\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Move model to AMD GPU\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "\n",
    "    print(f\"Model loaded successfully on {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {e}\")\n",
    "    print(\"Using a fallback smaller model for demonstration...\")\n",
    "\n",
    "    # Fallback to a smaller model that's publicly available\n",
    "    model_name = \"gpt2\"\n",
    "    from transformers import GPT2LMHeadModel\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Fallback model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd2af960cf04478a5225b1f8fc4205d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fd43b15d4041419130bb551150fab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cf9e524fa2415eb558889803fc649d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8a587cdebd417288b4d9867a5a4156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n",
      "Vocabulary size: 32000\n",
      "Special tokens: EOS=</s>, PAD=</s>\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer corresponding to the model\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Set padding token if not available\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Tokenizer loaded successfully\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Special tokens: EOS={tokenizer.eos_token}, PAD={tokenizer.pad_token}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Tokenizer loading failed: {e}\")\n",
    "    print(\"Using fallback tokenizer...\")\n",
    "\n",
    "    # Fallback tokenizer\n",
    "    from transformers import GPT2Tokenizer\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Fallback tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Tokenization and Processing\n",
    "\n",
    "Now we'll process input text through tokenization. This converts human-readable text into numerical tokens that the model can understand.\n",
    "\n",
    "**Tokenization Process:**\n",
    "1. **Text Input**: Raw text string\n",
    "2. **Encoding**: Convert text to token IDs using the tokenizer\n",
    "3. **Tensor Conversion**: Convert to PyTorch tensors\n",
    "4. **Device Transfer**: Move tensors to AMD GPU\n",
    "\n",
    "**Key Components:**\n",
    "- **input_ids**: Numerical representation of text tokens\n",
    "- **attention_mask**: Indicates which tokens should be attended to\n",
    "- **return_tensors**: Format of returned data (PyTorch tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "'Hello! Please introduce yourself and explain what you can do.'\n",
      "\n",
      "Tokenized input:\n",
      "Input IDs shape: torch.Size([1, 12])\n",
      "Input IDs: tensor([[15496,     0,  4222, 10400,  3511,   290,  4727,   644,   345,   460,\n",
      "           466,    13]], device='cuda:0')\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "\n",
      "Token breakdown:\n",
      "   0: 15496 -> 'Hello'\n",
      "   1:     0 -> '!'\n",
      "   2:  4222 -> 'ĠPlease'\n",
      "   3: 10400 -> 'Ġintroduce'\n",
      "   4:  3511 -> 'Ġyourself'\n",
      "   5:   290 -> 'Ġand'\n",
      "   6:  4727 -> 'Ġexplain'\n",
      "   7:   644 -> 'Ġwhat'\n",
      "   8:   345 -> 'Ġyou'\n",
      "   9:   460 -> 'Ġcan'\n",
      "  10:   466 -> 'Ġdo'\n",
      "  11:    13 -> '.'\n"
     ]
    }
   ],
   "source": [
    "# Process input text through tokenization\n",
    "prompt = \"Hello! Please introduce yourself and explain what you can do.\"\n",
    "\n",
    "# Use English prompt for better compatibility with most models\n",
    "selected_prompt = prompt\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(f\"'{selected_prompt}'\")\n",
    "print()\n",
    "\n",
    "# Tokenize the input\n",
    "input_data = tokenizer(selected_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move to AMD GPU\n",
    "input_ids = input_data[\"input_ids\"].to(device)\n",
    "attention_mask = input_data[\"attention_mask\"].to(device)\n",
    "\n",
    "print(\"Tokenized input:\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Attention mask: {attention_mask}\")\n",
    "print()\n",
    "\n",
    "# Show token-to-text mapping\n",
    "print(\"Token breakdown:\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "for i, (token_id, token) in enumerate(zip(input_ids[0], tokens)):\n",
    "    print(f\"  {i:2d}: {token_id:5d} -> '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation\n",
    "\n",
    "Now we'll use the model to generate text based on our input prompt. The model will predict the next tokens autoregressively.\n",
    "\n",
    "**Generation Process:**\n",
    "1. **Input Processing**: Feed tokenized input to the model\n",
    "2. **Forward Pass**: Model computes probability distributions over vocabulary\n",
    "3. **Token Sampling**: Select next tokens based on probabilities\n",
    "4. **Autoregressive Generation**: Repeat process for multiple tokens\n",
    "\n",
    "**Generation Parameters:**\n",
    "- **max_length**: Maximum number of tokens to generate\n",
    "- **temperature**: Controls randomness (lower = more deterministic)\n",
    "- **do_sample**: Whether to use sampling vs greedy decoding\n",
    "- **pad_token_id**: Token used for padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text on AMD GPU...\n",
      "Text generation completed!\n",
      "Generated tensor shape: torch.Size([1, 62])\n",
      "Generated IDs: tensor([[15496,     0,  4222, 10400,  3511,   290,  4727,   644,   345,   460,\n",
      "           466,    13,   314,  1183,  1826,   351,   257,  1178,   286,   616,\n",
      "          7810,   379,   262,  2059,   357,   732,   869,   606,   705,   464,\n",
      "          4380, 33809,   508,   481,   307,  1498,   284,  1037,   514,   503,\n",
      "            11,   655,   416,  4737,  2683,   546,  2972,  1243,   326,   356,\n",
      "           892,   389,  1593,   329,  4152,  2444,   553,   339,   531,    11,\n",
      "          4375,   326]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the model\n",
    "print(\"Generating text on AMD GPU...\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    try:\n",
    "        # Generate text with controlled parameters\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=100,  # Maximum total length (input + generated)\n",
    "            max_new_tokens=50,  # Maximum new tokens to generate\n",
    "            temperature=0.7,  # Control randomness\n",
    "            do_sample=True,  # Use sampling instead of greedy\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,  # Reduce repetition\n",
    "            no_repeat_ngram_size=3,  # Avoid repeating 3-grams\n",
    "        )\n",
    "\n",
    "        print(\"Text generation completed!\")\n",
    "        print(f\"Generated tensor shape: {generated_ids.shape}\")\n",
    "        print(f\"Generated IDs: {generated_ids}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Generation failed: {e}\")\n",
    "        # Fallback to simpler generation\n",
    "        generated_ids = model.generate(input_ids, max_length=60, pad_token_id=tokenizer.pad_token_id)\n",
    "        print(\"Fallback generation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output Decoding and Analysis\n",
    "\n",
    "Finally, we'll convert the generated token IDs back to human-readable text and analyze the results.\n",
    "\n",
    "**Decoding Process:**\n",
    "1. **Token to Text**: Convert generated IDs back to text using tokenizer\n",
    "2. **Special Token Handling**: Remove or handle special tokens appropriately\n",
    "3. **Post-processing**: Clean up the output text\n",
    "4. **Analysis**: Examine the generation quality and characteristics\n",
    "\n",
    "**Key Considerations:**\n",
    "- **skip_special_tokens**: Whether to remove special tokens from output\n",
    "- **clean_up_tokenization_spaces**: Handle tokenization artifacts\n",
    "- **Batch processing**: Handle multiple sequences if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding generated text...\n",
      "Results:\n",
      "==================================================\n",
      "Original prompt: 'Hello! Please introduce yourself and explain what you can do.'\n",
      "==================================================\n",
      "Full output: Hello! Please introduce yourself and explain what you can do. I'll meet with a few of my colleagues at the University (we call them 'The People'), who will be able to help us out, just by asking questions about various things that we think are important for college students,\" he said, adding that\n",
      "==================================================\n",
      "Generated text only: ' I'll meet with a few of my colleagues at the University (we call them 'The People'), who will be able to help us out, just by asking questions about various things that we think are important for college students,\" he said, adding that'\n",
      "==================================================\n",
      "\n",
      "Generation Analysis:\n",
      "Input tokens: 12\n",
      "Generated tokens: 50\n",
      "Total tokens: 62\n",
      "Generated text length: 236 characters\n",
      "\n",
      "Performance:\n",
      "Device used: cuda\n",
      "Model dtype: torch.float32\n",
      "Lab 1 completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated tokens back to text\n",
    "print(\"Decoding generated text...\")\n",
    "\n",
    "# Decode the full sequence (input + generated)\n",
    "full_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Extract only the generated part (remove input prompt)\n",
    "input_length = input_ids.shape[1]\n",
    "generated_only_ids = generated_ids[:, input_length:]\n",
    "\n",
    "generated_text = tokenizer.batch_decode(generated_only_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(\"Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original prompt: '{selected_prompt}'\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Full output: {full_output[0]}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Generated text only: '{generated_text[0]}'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analysis\n",
    "print(\"\\nGeneration Analysis:\")\n",
    "print(f\"Input tokens: {input_length}\")\n",
    "print(f\"Generated tokens: {generated_only_ids.shape[1]}\")\n",
    "print(f\"Total tokens: {generated_ids.shape[1]}\")\n",
    "print(f\"Generated text length: {len(generated_text[0])} characters\")\n",
    "\n",
    "# Performance info\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"Device used: {device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(\"Lab 1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "### Technical Concepts Learned\n",
    "- **Transformer Architecture**: Understanding of autoregressive text generation\n",
    "- **Tokenization Process**: Text to token conversion and vice versa\n",
    "- **Model Loading**: Loading and configuring pre-trained LLM models\n",
    "- **Text Generation**: Generating coherent text using sampling strategies\n",
    "- **Model Inference Pipeline**: Complete workflow from input to output\n",
    "\n",
    "### Experiment Further\n",
    "- Change the `temperature` value (0.1 to 2.0) to control randomness\n",
    "- Adjust `max_new_tokens` for longer/shorter outputs\n",
    "- Experiment with different prompts\n",
    "- Try the Chinese prompt for multilingual capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
