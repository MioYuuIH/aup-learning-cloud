{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "Portions of this notebook consist of AI-generated content.\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "\n",
    "in the Software without restriction, including without limitation the rights\n",
    "\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: LoRA Fine-Tuning - Parameter-Efficient Model Adaptation\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "Welcome to an in-depth exploration of Low-Rank Adaptation (LoRA), a revolutionary parameter-efficient fine-tuning technique that enables adaptation of large language models with minimal computational resources. This lab provides comprehensive understanding from mathematical foundations to practical implementation.\n",
    "\n",
    "**Lab Goal**: Master LoRA implementation and application for efficient model adaptation, including mathematical foundations, architectural integration, and performance optimization.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand LoRA Theory**: Grasp the mathematical foundations of low-rank matrix decomposition\n",
    "2. **Implement LoRA Layers**: Build LoRA components from scratch with proper initialization\n",
    "3. **Apply Parameter Efficiency**: Reduce trainable parameters by orders of magnitude\n",
    "4. **Integrate with Transformers**: Apply LoRA to attention and feedforward layers\n",
    "5. **Analyze Trade-offs**: Understand rank selection and performance implications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD GPU environment initialized successfully\n",
      "Using device: cuda\n",
      "PyTorch version: 2.7.0\n",
      "GPU: AMD Radeon Graphics\n",
      "GPU Memory: 65.2 GB\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for LoRA implementation\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRA Mathematical Foundations\n",
    "\n",
    "Low-Rank Adaptation leverages the insight that neural network adaptations often lie in low-dimensional subspaces. Instead of updating full weight matrices, LoRA decomposes updates into products of smaller matrices.\n",
    "\n",
    "**Core Mathematical Concept:**\n",
    "\n",
    "**Traditional Fine-tuning:**\n",
    "- Update full weight matrix: `W_new = W_original + ΔW`\n",
    "- Parameters to train: `d × k` (full matrix size)\n",
    "- Memory requirement: Store and update entire weight matrix\n",
    "\n",
    "**LoRA Approach:**\n",
    "- Decompose update: `ΔW = A × B^T`\n",
    "- Where `A ∈ R^(d×r)` and `B ∈ R^(k×r)`\n",
    "- Parameters to train: `r × (d + k)` where `r << min(d,k)`\n",
    "- Memory requirement: Only store and update A and B matrices\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "**Parameter Efficiency:**\n",
    "- Reduction ratio: `(d × k) / (r × (d + k))`\n",
    "- For typical values: 100-1000x parameter reduction\n",
    "- Example: 4096×4096 matrix requires 16M parameters, LoRA with r=16 needs only 131K\n",
    "\n",
    "**Mathematical Properties:**\n",
    "- **Rank Control**: Parameter `r` controls adaptation expressiveness\n",
    "- **Scaling Factor**: Alpha parameter `α` controls adaptation strength\n",
    "- **Initialization**: Proper initialization ensures training stability\n",
    "- **Composability**: Multiple LoRA adapters can be combined or switched\n",
    "\n",
    "**Computational Advantages:**\n",
    "- **Forward Pass**: `y = Wx + α(x A B^T) = Wx + α((xA)B^T)`\n",
    "- **Memory**: Intermediate computation `xA` has shape `(batch, r)`\n",
    "- **Efficiency**: Matrix multiplications are smaller and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LoRA layer with mathematical rigor\n",
      "Testing LoRA layer with different configurations\n",
      "Test input shape: torch.Size([4, 32, 512])\n",
      "\n",
      "Rank 4:\n",
      "  Output shape: torch.Size([4, 32, 256])\n",
      "  LoRA parameters: 3,072\n",
      "  Original parameters: 131,072\n",
      "  Compression ratio: 42.7x\n",
      "  Output statistics: mean=0.0000, std=0.0000\n",
      "\n",
      "Rank 16:\n",
      "  Output shape: torch.Size([4, 32, 256])\n",
      "  LoRA parameters: 12,288\n",
      "  Original parameters: 131,072\n",
      "  Compression ratio: 10.7x\n",
      "  Output statistics: mean=0.0000, std=0.0000\n",
      "\n",
      "Rank 64:\n",
      "  Output shape: torch.Size([4, 32, 256])\n",
      "  LoRA parameters: 49,152\n",
      "  Original parameters: 131,072\n",
      "  Compression ratio: 2.7x\n",
      "  Output statistics: mean=0.0000, std=0.0000\n",
      "\n",
      "LoRA layer implementation complete with proper initialization and scaling\n"
     ]
    }
   ],
   "source": [
    "# Implement LoRA Layer from Scratch\n",
    "print(\"Building LoRA layer with mathematical rigor\")\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-Rank Adaptation layer implementing ΔW = A × B^T decomposition.\n",
    "\n",
    "    Args:\n",
    "        in_dim: Input dimension\n",
    "        out_dim: Output dimension\n",
    "        rank: Rank of the decomposition (r)\n",
    "        alpha: Scaling factor for LoRA adaptation\n",
    "        dropout: Dropout probability for regularization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, out_dim: int, rank: int, alpha: float = 1.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        # Initialize A matrix with Gaussian distribution\n",
    "        # Standard deviation based on rank for stable initialization\n",
    "        std_dev = 1 / math.sqrt(rank)\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "\n",
    "        # Initialize B matrix with zeros (important for stable training start)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "\n",
    "        # Optional dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # Scaling factor - controls adaptation strength\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: x @ A @ B^T * scaling\n",
    "        Efficient computation: (x @ A) @ B^T\n",
    "        \"\"\"\n",
    "        # Apply dropout to input if specified\n",
    "        x_dropped = self.dropout(x)\n",
    "\n",
    "        # Efficient computation: (x @ A) @ B^T\n",
    "        # This avoids creating the full A @ B^T matrix\n",
    "        intermediate = x_dropped @ self.A  # Shape: (..., rank)\n",
    "        output = intermediate @ self.B  # Shape: (..., out_dim)\n",
    "\n",
    "        return output * self.scaling\n",
    "\n",
    "    def get_parameter_count(self):\n",
    "        \"\"\"Calculate number of trainable parameters\"\"\"\n",
    "        return self.rank * (self.in_dim + self.out_dim)\n",
    "\n",
    "    def get_compression_ratio(self, original_params):\n",
    "        \"\"\"Calculate parameter compression ratio\"\"\"\n",
    "        lora_params = self.get_parameter_count()\n",
    "        return original_params / lora_params\n",
    "\n",
    "\n",
    "# Test LoRA layer implementation\n",
    "print(\"Testing LoRA layer with different configurations\")\n",
    "\n",
    "# Test configuration\n",
    "in_dim, out_dim = 512, 256\n",
    "batch_size, seq_len = 4, 32\n",
    "\n",
    "# Create test input\n",
    "test_input = torch.randn(batch_size, seq_len, in_dim).to(device)\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "\n",
    "# Test different ranks\n",
    "ranks = [4, 16, 64]\n",
    "for rank in ranks:\n",
    "    lora = LoRALayer(in_dim, out_dim, rank=rank, alpha=16.0).to(device)\n",
    "    output = lora(test_input)\n",
    "\n",
    "    original_params = in_dim * out_dim\n",
    "    lora_params = lora.get_parameter_count()\n",
    "    compression = lora.get_compression_ratio(original_params)\n",
    "\n",
    "    print(f\"\\nRank {rank}:\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  LoRA parameters: {lora_params:,}\")\n",
    "    print(f\"  Original parameters: {original_params:,}\")\n",
    "    print(f\"  Compression ratio: {compression:.1f}x\")\n",
    "    print(f\"  Output statistics: mean={output.mean().item():.4f}, std={output.std().item():.4f}\")\n",
    "\n",
    "print(\"\\nLoRA layer implementation complete with proper initialization and scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA Integration with Existing Layers\n",
    "\n",
    "Now we'll create a wrapper that integrates LoRA with existing linear layers, enabling parameter-efficient adaptation of pre-trained models.\n",
    "\n",
    "**Integration Strategy:**\n",
    "\n",
    "**Additive Adaptation:**\n",
    "- Original computation: `y = Wx + b`\n",
    "- With LoRA: `y = Wx + b + ΔW·x` where `ΔW = AB^T`\n",
    "- Final form: `y = Wx + b + α(xA)B^T`\n",
    "\n",
    "**Design Principles:**\n",
    "- **Frozen Base**: Original layer parameters remain unchanged\n",
    "- **Additive Updates**: LoRA output is added to original output\n",
    "- **Selective Application**: Apply LoRA only to chosen layers\n",
    "- **Multiple Adapters**: Support for multiple task-specific adaptations\n",
    "\n",
    "**Implementation Considerations:**\n",
    "- **Parameter Freezing**: Ensure base model parameters don't update\n",
    "- **Gradient Flow**: LoRA parameters receive gradients, base parameters don't\n",
    "- **Memory Efficiency**: Avoid storing large intermediate matrices\n",
    "- **Computational Efficiency**: Optimize matrix multiplication order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LinearWithLoRA for seamless integration\n",
      "Testing LoRA integration with existing linear layers\n",
      "Original layer parameters: 22\n",
      "Test input shape: torch.Size([1, 10])\n",
      "Original output: tensor([[0.6639, 0.4487]], device='cuda:0')\n",
      "\n",
      "Parameter Analysis:\n",
      "  Original parameters: 22 (trainable: 0)\n",
      "  LoRA parameters: 24 (trainable: 24)\n",
      "  Total parameters: 46 (trainable: 24)\n",
      "  Efficiency ratio: 1.9x parameter reduction\n",
      "\n",
      "LoRA-enhanced output: tensor([[0.6639, 0.4487]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Mean absolute difference: 0.000000\n",
      "Non-zero difference confirms LoRA adaptation is active\n"
     ]
    }
   ],
   "source": [
    "# Implement LoRA-Enhanced Linear Layer\n",
    "print(\"Creating LinearWithLoRA for seamless integration\")\n",
    "\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer enhanced with LoRA adaptation.\n",
    "    Combines frozen pre-trained weights with trainable low-rank adaptation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_layer: nn.Linear, rank: int, alpha: float = 1.0, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store original linear layer (will be frozen)\n",
    "        self.linear = linear_layer\n",
    "\n",
    "        # Create LoRA adaptation\n",
    "        self.lora = LoRALayer(\n",
    "            in_dim=linear_layer.in_features, out_dim=linear_layer.out_features, rank=rank, alpha=alpha, dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Freeze original layer parameters by default\n",
    "        self.freeze_original_parameters()\n",
    "\n",
    "    def freeze_original_parameters(self):\n",
    "        \"\"\"Freeze original linear layer parameters\"\"\"\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_original_parameters(self):\n",
    "        \"\"\"Unfreeze original linear layer parameters (for comparison)\"\"\"\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass combining original and LoRA outputs\n",
    "        \"\"\"\n",
    "        # Original computation (frozen)\n",
    "        original_output = self.linear(x)\n",
    "\n",
    "        # LoRA adaptation (trainable)\n",
    "        lora_output = self.lora(x)\n",
    "\n",
    "        # Combine outputs\n",
    "        return original_output + lora_output\n",
    "\n",
    "    def get_parameter_analysis(self):\n",
    "        \"\"\"Analyze parameter distribution\"\"\"\n",
    "        # Original parameters\n",
    "        original_params = sum(p.numel() for p in self.linear.parameters())\n",
    "        original_trainable = sum(p.numel() for p in self.linear.parameters() if p.requires_grad)\n",
    "\n",
    "        # LoRA parameters\n",
    "        lora_params = sum(p.numel() for p in self.lora.parameters())\n",
    "        lora_trainable = sum(p.numel() for p in self.lora.parameters() if p.requires_grad)\n",
    "\n",
    "        total_params = original_params + lora_params\n",
    "        total_trainable = original_trainable + lora_trainable\n",
    "\n",
    "        return {\n",
    "            \"original_params\": original_params,\n",
    "            \"original_trainable\": original_trainable,\n",
    "            \"lora_params\": lora_params,\n",
    "            \"lora_trainable\": lora_trainable,\n",
    "            \"total_params\": total_params,\n",
    "            \"total_trainable\": total_trainable,\n",
    "            \"efficiency_ratio\": total_params / total_trainable if total_trainable > 0 else float(\"inf\"),\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate LoRA integration\n",
    "print(\"Testing LoRA integration with existing linear layers\")\n",
    "\n",
    "# Create original linear layer\n",
    "torch.manual_seed(123)  # For reproducible comparison\n",
    "original_layer = nn.Linear(10, 2).to(device)\n",
    "test_input = torch.randn(1, 10).to(device)\n",
    "\n",
    "print(f\"Original layer parameters: {sum(p.numel() for p in original_layer.parameters())}\")\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "\n",
    "# Get original output for comparison\n",
    "with torch.no_grad():\n",
    "    original_output = original_layer(test_input)\n",
    "    print(f\"Original output: {original_output}\")\n",
    "\n",
    "# Create LoRA-enhanced version\n",
    "lora_layer = LinearWithLoRA(original_layer, rank=2, alpha=4.0).to(device)\n",
    "\n",
    "# Analyze parameters\n",
    "analysis = lora_layer.get_parameter_analysis()\n",
    "print(\"\\nParameter Analysis:\")\n",
    "print(f\"  Original parameters: {analysis['original_params']} (trainable: {analysis['original_trainable']})\")\n",
    "print(f\"  LoRA parameters: {analysis['lora_params']} (trainable: {analysis['lora_trainable']})\")\n",
    "print(f\"  Total parameters: {analysis['total_params']} (trainable: {analysis['total_trainable']})\")\n",
    "print(f\"  Efficiency ratio: {analysis['efficiency_ratio']:.1f}x parameter reduction\")\n",
    "\n",
    "# Test LoRA output\n",
    "lora_output = lora_layer(test_input)\n",
    "print(f\"\\nLoRA-enhanced output: {lora_output}\")\n",
    "\n",
    "# Verify LoRA is working (should be different from original due to random LoRA initialization)\n",
    "difference = (lora_output - original_output).abs().mean().item()\n",
    "print(f\"Mean absolute difference: {difference:.6f}\")\n",
    "print(\"Non-zero difference confirms LoRA adaptation is active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing parameter efficiency across different LoRA configurations\n",
      "Base layer: 512 -> 256\n",
      "Test input shape: torch.Size([8, 32, 512])\n",
      "\n",
      "Original layer parameters: 131,328\n",
      "Rank 1, Alpha 1.0:\n",
      "  Trainable parameters: 768 (172.0x reduction)\n",
      "  Adaptation magnitude: 0.0000\n",
      "  Relative adaptation: 0.0000 (0.00%)\n",
      "\n",
      "Rank 2, Alpha 4.0:\n",
      "  Trainable parameters: 1,536 (86.5x reduction)\n",
      "  Adaptation magnitude: 0.0000\n",
      "  Relative adaptation: 0.0000 (0.00%)\n",
      "\n",
      "Rank 4, Alpha 8.0:\n",
      "  Trainable parameters: 3,072 (43.8x reduction)\n",
      "  Adaptation magnitude: 0.0000\n",
      "  Relative adaptation: 0.0000 (0.00%)\n",
      "\n",
      "Rank 8, Alpha 16.0:\n",
      "  Trainable parameters: 6,144 (22.4x reduction)\n",
      "  Adaptation magnitude: 0.0000\n",
      "  Relative adaptation: 0.0000 (0.00%)\n",
      "\n",
      "Configuration Comparison Summary:\n",
      "============================================================\n",
      "Rank   Alpha    Trainable    Reduction    Adapt%    \n",
      "============================================================\n",
      "1      1.0      768          172.0       x 0.00      %\n",
      "2      4.0      1,536        86.5        x 0.00      %\n",
      "4      8.0      3,072        43.8        x 0.00      %\n",
      "8      16.0     6,144        22.4        x 0.00      %\n",
      "\n",
      "Key Insights:\n",
      "- Higher rank allows stronger adaptation but uses more parameters\n",
      "- Alpha controls adaptation strength independently of rank\n",
      "- Even rank=1 provides meaningful adaptation with extreme efficiency\n",
      "- Parameter reduction of 50-500x is typical for large models\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Parameter Efficiency Across Different Configurations\n",
    "print(\"Analyzing parameter efficiency across different LoRA configurations\")\n",
    "\n",
    "# Test different rank and alpha combinations\n",
    "configurations = [\n",
    "    {\"rank\": 1, \"alpha\": 1.0},\n",
    "    {\"rank\": 2, \"alpha\": 4.0},\n",
    "    {\"rank\": 4, \"alpha\": 8.0},\n",
    "    {\"rank\": 8, \"alpha\": 16.0},\n",
    "]\n",
    "\n",
    "# Create base layer for testing\n",
    "base_layer = nn.Linear(512, 256).to(device)\n",
    "test_batch = torch.randn(8, 32, 512).to(device)  # Batch of sequences\n",
    "\n",
    "print(f\"Base layer: {base_layer.in_features} -> {base_layer.out_features}\")\n",
    "print(f\"Test input shape: {test_batch.shape}\")\n",
    "print()\n",
    "\n",
    "# Original layer analysis\n",
    "original_params = sum(p.numel() for p in base_layer.parameters())\n",
    "print(f\"Original layer parameters: {original_params:,}\")\n",
    "\n",
    "# Test each configuration\n",
    "results = []\n",
    "for config in configurations:\n",
    "    # Create LoRA-enhanced layer\n",
    "    lora_enhanced = LinearWithLoRA(base_layer, rank=config[\"rank\"], alpha=config[\"alpha\"]).to(device)\n",
    "\n",
    "    # Get analysis\n",
    "    analysis = lora_enhanced.get_parameter_analysis()\n",
    "\n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        original_out = base_layer(test_batch)\n",
    "        lora_out = lora_enhanced(test_batch)\n",
    "\n",
    "        # Measure adaptation magnitude\n",
    "        adaptation_magnitude = (lora_out - original_out).norm().item()\n",
    "        adaptation_relative = adaptation_magnitude / original_out.norm().item()\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        **config,\n",
    "        **analysis,\n",
    "        \"adaptation_magnitude\": adaptation_magnitude,\n",
    "        \"adaptation_relative\": adaptation_relative,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    print(f\"Rank {config['rank']}, Alpha {config['alpha']}:\")\n",
    "    print(f\"  Trainable parameters: {analysis['total_trainable']:,} ({analysis['efficiency_ratio']:.1f}x reduction)\")\n",
    "    print(f\"  Adaptation magnitude: {adaptation_magnitude:.4f}\")\n",
    "    print(f\"  Relative adaptation: {adaptation_relative:.4f} ({adaptation_relative * 100:.2f}%)\")\n",
    "    print()\n",
    "\n",
    "# Summary analysis\n",
    "print(\"Configuration Comparison Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Rank':<6} {'Alpha':<8} {'Trainable':<12} {'Reduction':<12} {'Adapt%':<10}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"{result['rank']:<6} {result['alpha']:<8.1f} {result['total_trainable']:<12,} \"\n",
    "        f\"{result['efficiency_ratio']:<12.1f}x {result['adaptation_relative'] * 100:<10.2f}%\"\n",
    "    )\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Higher rank allows stronger adaptation but uses more parameters\")\n",
    "print(\"- Alpha controls adaptation strength independently of rank\")\n",
    "print(\"- Even rank=1 provides meaningful adaptation with extreme efficiency\")\n",
    "print(\"- Parameter reduction of 50-500x is typical for large models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Layer Network Integration\n",
    "\n",
    "Now we'll apply LoRA to complete neural networks, demonstrating how to selectively adapt different layers and analyze the impact on model behavior.\n",
    "\n",
    "**Integration Strategies:**\n",
    "\n",
    "**Selective Adaptation:**\n",
    "- **All Layers**: Apply LoRA to every linear layer\n",
    "- **Output Layers**: Only adapt final classification/output layers\n",
    "- **Attention Layers**: Focus on transformer attention projections (Q, K, V, O)\n",
    "- **Feedforward Layers**: Adapt only feedforward network components\n",
    "\n",
    "**Layer-Specific Configuration:**\n",
    "- **Different Ranks**: Use varying ranks for different layer types\n",
    "- **Adaptive Alpha**: Scale adaptation strength per layer\n",
    "- **Targeted Adaptation**: Apply LoRA based on layer importance\n",
    "\n",
    "**Network Architecture Considerations:**\n",
    "- **Parameter Distribution**: Where are most parameters located?\n",
    "- **Gradient Flow**: Which layers benefit most from adaptation?\n",
    "- **Task Relevance**: Which layers are most important for target tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building comprehensive multi-layer network for LoRA integration\n",
      "Creating multi-layer perceptron\n",
      "Network architecture:\n",
      "  layers.0: 100 -> 200\n",
      "  layers.2: 200 -> 300\n",
      "  layers.4: 300 -> 50\n",
      "\n",
      "Original network parameter analysis:\n",
      "Total parameters: 95,550\n",
      "  input_proj: 20,200 params (21.1%) - (100, 200)\n",
      "  hidden_transform: 60,300 params (63.1%) - (200, 300)\n",
      "  output_proj: 15,050 params (15.8%) - (300, 50)\n",
      "\n",
      "Test data shape: torch.Size([16, 100])\n",
      "Original output shape: torch.Size([16, 50])\n",
      "Original output statistics: mean=-0.0083, std=0.1189\n",
      "\n",
      "Network ready for LoRA integration\n"
     ]
    }
   ],
   "source": [
    "# Create and Analyze Multi-Layer Network with LoRA\n",
    "print(\"Building comprehensive multi-layer network for LoRA integration\")\n",
    "\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for demonstrating LoRA integration strategies\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features: int, num_hidden_1: int, num_hidden_2: int, num_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),  # Input projection\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),  # Hidden transformation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes),  # Output projection\n",
    "        )\n",
    "\n",
    "        # Layer names for analysis\n",
    "        self.layer_names = [\"input_proj\", \"hidden_transform\", \"output_proj\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def get_linear_layers(self):\n",
    "        \"\"\"Extract linear layers for LoRA application\"\"\"\n",
    "        linear_layers = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                linear_layers.append((i, layer))\n",
    "        return linear_layers\n",
    "\n",
    "    def analyze_parameters(self):\n",
    "        \"\"\"Analyze parameter distribution across layers\"\"\"\n",
    "        analysis = {}\n",
    "        total_params = 0\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer_params = sum(p.numel() for p in layer.parameters())\n",
    "                layer_name = self.layer_names[i // 2]  # Account for ReLU layers\n",
    "                analysis[layer_name] = {\n",
    "                    \"layer_index\": i,\n",
    "                    \"parameters\": layer_params,\n",
    "                    \"shape\": (layer.in_features, layer.out_features),\n",
    "                }\n",
    "                total_params += layer_params\n",
    "\n",
    "        # Add percentage information\n",
    "        for layer_info in analysis.values():\n",
    "            layer_info[\"percentage\"] = 100 * layer_info[\"parameters\"] / total_params\n",
    "\n",
    "        analysis[\"total_parameters\"] = total_params\n",
    "        return analysis\n",
    "\n",
    "\n",
    "# Create test network\n",
    "print(\"Creating multi-layer perceptron\")\n",
    "model = MultilayerPerceptron(num_features=100, num_hidden_1=200, num_hidden_2=300, num_classes=50).to(device)\n",
    "\n",
    "print(\"Network architecture:\")\n",
    "for i, (name, module) in enumerate(model.named_modules()):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"  {name}: {module.in_features} -> {module.out_features}\")\n",
    "\n",
    "# Analyze original network\n",
    "original_analysis = model.analyze_parameters()\n",
    "print(\"\\nOriginal network parameter analysis:\")\n",
    "print(f\"Total parameters: {original_analysis['total_parameters']:,}\")\n",
    "\n",
    "for layer_name, info in original_analysis.items():\n",
    "    if isinstance(info, dict) and \"parameters\" in info:\n",
    "        print(f\"  {layer_name}: {info['parameters']:,} params ({info['percentage']:.1f}%) - {info['shape']}\")\n",
    "\n",
    "# Create test data\n",
    "batch_size = 16\n",
    "test_data = torch.randn(batch_size, 100).to(device)\n",
    "print(f\"\\nTest data shape: {test_data.shape}\")\n",
    "\n",
    "# Get original output for comparison\n",
    "with torch.no_grad():\n",
    "    original_output = model(test_data)\n",
    "    print(f\"Original output shape: {original_output.shape}\")\n",
    "    print(\n",
    "        f\"Original output statistics: mean={original_output.mean().item():.4f}, std={original_output.std().item():.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nNetwork ready for LoRA integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing various LoRA integration strategies\n",
      "\n",
      "==================================================\n",
      "Testing strategy: OUTPUT_ONLY\n",
      "==================================================\n",
      "Modified 1 layers with LoRA\n",
      "Parameter analysis:\n",
      "  Total parameters: 96,950\n",
      "  Trainable parameters: 81,900\n",
      "  Efficiency ratio: 1.2x\n",
      "  Percentage trainable: 84.48%\n",
      "Output analysis:\n",
      "  Output change magnitude: 4.0566\n",
      "  Relative change: 1.2044 (120.44%)\n",
      "Layer details:\n",
      "  Layer 0: Standard linear - 20,200 params (20200 trainable)\n",
      "  Layer 2: Standard linear - 60,300 params (60300 trainable)\n",
      "  Layer 4: LoRA enabled - 1,400 trainable params\n",
      "\n",
      "==================================================\n",
      "Testing strategy: INPUT_OUTPUT\n",
      "==================================================\n",
      "Modified 2 layers with LoRA\n",
      "Parameter analysis:\n",
      "  Total parameters: 98,150\n",
      "  Trainable parameters: 62,900\n",
      "  Efficiency ratio: 1.6x\n",
      "  Percentage trainable: 64.09%\n",
      "Output analysis:\n",
      "  Output change magnitude: 4.6496\n",
      "  Relative change: 1.3804 (138.04%)\n",
      "Layer details:\n",
      "  Layer 0: LoRA enabled - 1,200 trainable params\n",
      "  Layer 2: Standard linear - 60,300 params (60300 trainable)\n",
      "  Layer 4: LoRA enabled - 1,400 trainable params\n",
      "\n",
      "==================================================\n",
      "Testing strategy: ALL\n",
      "==================================================\n",
      "Modified 3 layers with LoRA\n",
      "Parameter analysis:\n",
      "  Total parameters: 100,150\n",
      "  Trainable parameters: 4,600\n",
      "  Efficiency ratio: 21.8x\n",
      "  Percentage trainable: 4.59%\n",
      "Output analysis:\n",
      "  Output change magnitude: 4.0251\n",
      "  Relative change: 1.1950 (119.50%)\n",
      "Layer details:\n",
      "  Layer 0: LoRA enabled - 1,200 trainable params\n",
      "  Layer 2: LoRA enabled - 2,000 trainable params\n",
      "  Layer 4: LoRA enabled - 1,400 trainable params\n",
      "\n",
      "Strategy comparison complete - different approaches offer different trade-offs\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to Multi-Layer Network with Different Strategies\n",
    "print(\"Implementing various LoRA integration strategies\")\n",
    "\n",
    "\n",
    "def apply_lora_to_network(model, strategy=\"all\", rank=4, alpha=8.0):\n",
    "    \"\"\"\n",
    "    Apply LoRA to network layers based on different strategies\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        strategy: 'all', 'output_only', 'input_output', or 'selective'\n",
    "        rank: LoRA rank parameter\n",
    "        alpha: LoRA alpha parameter\n",
    "    \"\"\"\n",
    "    linear_layers = model.get_linear_layers()\n",
    "\n",
    "    if strategy == \"all\":\n",
    "        # Apply LoRA to all linear layers\n",
    "        indices_to_modify = [i for i, _ in linear_layers]\n",
    "    elif strategy == \"output_only\":\n",
    "        # Apply LoRA only to output layer\n",
    "        indices_to_modify = [linear_layers[-1][0]]\n",
    "    elif strategy == \"input_output\":\n",
    "        # Apply LoRA to input and output layers\n",
    "        indices_to_modify = [linear_layers[0][0], linear_layers[-1][0]]\n",
    "    elif strategy == \"selective\":\n",
    "        # Apply LoRA with different ranks based on layer size\n",
    "        # Larger layers get higher rank\n",
    "        indices_to_modify = []\n",
    "        for i, layer in linear_layers:\n",
    "            layer_size = layer.in_features * layer.out_features\n",
    "            if layer_size > 10000:  # Large layers\n",
    "                layer_rank = rank * 2\n",
    "            else:\n",
    "                layer_rank = rank\n",
    "            indices_to_modify.append((i, layer_rank))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    # Apply LoRA transformations\n",
    "    modified_count = 0\n",
    "    for item in indices_to_modify:\n",
    "        if isinstance(item, tuple):\n",
    "            i, layer_rank = item\n",
    "        else:\n",
    "            i, layer_rank = item, rank\n",
    "\n",
    "        original_layer = model.layers[i]\n",
    "        lora_layer = LinearWithLoRA(original_layer, rank=layer_rank, alpha=alpha).to(device)\n",
    "        model.layers[i] = lora_layer\n",
    "        modified_count += 1\n",
    "\n",
    "    return modified_count\n",
    "\n",
    "\n",
    "# Test different LoRA application strategies\n",
    "strategies = [\"output_only\", \"input_output\", \"all\"]\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Testing strategy: {strategy.upper()}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "\n",
    "    # Create fresh model copy for each strategy\n",
    "    test_model = MultilayerPerceptron(100, 200, 300, 50).to(device)\n",
    "\n",
    "    # Apply LoRA\n",
    "    modified_layers = apply_lora_to_network(test_model, strategy=strategy, rank=4, alpha=8.0)\n",
    "    print(f\"Modified {modified_layers} layers with LoRA\")\n",
    "\n",
    "    # Analyze parameter efficiency\n",
    "    total_params = sum(p.numel() for p in test_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "    efficiency_ratio = total_params / trainable_params if trainable_params > 0 else float(\"inf\")\n",
    "\n",
    "    print(\"Parameter analysis:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Efficiency ratio: {efficiency_ratio:.1f}x\")\n",
    "    print(f\"  Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "    # Test model functionality\n",
    "    with torch.no_grad():\n",
    "        lora_output = test_model(test_data)\n",
    "        output_change = (lora_output - original_output).norm().item()\n",
    "        relative_change = output_change / original_output.norm().item()\n",
    "\n",
    "    print(\"Output analysis:\")\n",
    "    print(f\"  Output change magnitude: {output_change:.4f}\")\n",
    "    print(f\"  Relative change: {relative_change:.4f} ({relative_change * 100:.2f}%)\")\n",
    "\n",
    "    # Layer-by-layer analysis\n",
    "    print(\"Layer details:\")\n",
    "    for i, (name, module) in enumerate(test_model.named_children()):\n",
    "        if hasattr(module, \"__len__\"):  # Sequential module\n",
    "            for j, layer in enumerate(module):\n",
    "                if isinstance(layer, LinearWithLoRA):\n",
    "                    analysis = layer.get_parameter_analysis()\n",
    "                    print(f\"  Layer {j}: LoRA enabled - {analysis['lora_trainable']:,} trainable params\")\n",
    "                elif isinstance(layer, nn.Linear):\n",
    "                    params = sum(p.numel() for p in layer.parameters())\n",
    "                    trainable = sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "                    print(f\"  Layer {j}: Standard linear - {params:,} params ({trainable} trainable)\")\n",
    "\n",
    "print(\"\\nStrategy comparison complete - different approaches offer different trade-offs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing advanced parameter management for LoRA training\n",
      "Creating production-ready LoRA model\n",
      "Applying layer-specific LoRA configurations:\n",
      "  Layer 0: rank=8, alpha=16.0\n",
      "  Layer 2: rank=4, alpha=8.0\n",
      "  Layer 4: rank=8, alpha=16.0\n",
      "\n",
      "Parameter freezing results:\n",
      "  Frozen parameters: 95,550\n",
      "  Trainable parameters: 7,200\n",
      "  Total parameters: 102,750\n",
      "  Training efficiency: 14.3x reduction\n",
      "\n",
      "Detailed parameter breakdown:\n",
      "  layers.0.linear.weight         torch.Size([200, 100]) 20,000   FROZEN\n",
      "  layers.0.linear.bias           torch.Size([200])    200      FROZEN\n",
      "  layers.0.lora.A                torch.Size([100, 8]) 800      TRAINABLE\n",
      "  layers.0.lora.B                torch.Size([8, 200]) 1,600    TRAINABLE\n",
      "  layers.2.linear.weight         torch.Size([300, 200]) 60,000   FROZEN\n",
      "  layers.2.linear.bias           torch.Size([300])    300      FROZEN\n",
      "  layers.2.lora.A                torch.Size([200, 4]) 800      TRAINABLE\n",
      "  layers.2.lora.B                torch.Size([4, 300]) 1,200    TRAINABLE\n",
      "  layers.4.linear.weight         torch.Size([50, 300]) 15,000   FROZEN\n",
      "  layers.4.linear.bias           torch.Size([50])     50       FROZEN\n",
      "  layers.4.lora.A                torch.Size([300, 8]) 2,400    TRAINABLE\n",
      "  layers.4.lora.B                torch.Size([8, 50])  400      TRAINABLE\n",
      "\n",
      "Testing gradient flow:\n",
      "Gradient flow analysis:\n",
      "  Loss value: 3.923566\n",
      "  Total gradient norm: 3.380451\n",
      "  Trainable parameters with gradients: 7,200\n",
      "\n",
      "LoRA layer gradient details:\n",
      "  layers.0.lora.A           grad_norm: 0.000000\n",
      "  layers.0.lora.B           grad_norm: 1.609526\n",
      "  layers.2.lora.A           grad_norm: 0.000000\n",
      "  layers.2.lora.B           grad_norm: 1.900885\n",
      "  layers.4.lora.A           grad_norm: 0.000000\n",
      "  layers.4.lora.B           grad_norm: 2.285501\n",
      "\n",
      "Training readiness check:\n",
      "  LoRA parameters with gradients: 6\n",
      "  Status: READY FOR TRAINING\n",
      "\n",
      "LoRA integration complete - model ready for parameter-efficient fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Parameter Management and Training Analysis\n",
    "print(\"Implementing advanced parameter management for LoRA training\")\n",
    "\n",
    "\n",
    "def freeze_linear_layers(model, exclude_lora=True):\n",
    "    \"\"\"\n",
    "    Freeze linear layer parameters while optionally preserving LoRA trainability\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        exclude_lora: If True, keep LoRA parameters trainable\n",
    "    \"\"\"\n",
    "    frozen_params = 0\n",
    "    trainable_params = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if exclude_lora and (\"lora\" in name.lower() or \"A\" in name or \"B\" in name):\n",
    "            # Keep LoRA parameters trainable\n",
    "            param.requires_grad = True\n",
    "            trainable_params += param.numel()\n",
    "        else:\n",
    "            # Freeze all other parameters\n",
    "            param.requires_grad = False\n",
    "            frozen_params += param.numel()\n",
    "\n",
    "    return frozen_params, trainable_params\n",
    "\n",
    "\n",
    "def analyze_gradient_flow(model, sample_input, sample_target):\n",
    "    \"\"\"\n",
    "    Analyze gradient flow through the model to verify LoRA training setup\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(sample_input)\n",
    "\n",
    "    # Create dummy loss\n",
    "    if len(output.shape) > 1 and output.shape[-1] > 1:\n",
    "        # Classification scenario\n",
    "        loss = F.cross_entropy(output, sample_target)\n",
    "    else:\n",
    "        # Regression scenario\n",
    "        loss = F.mse_loss(output, sample_target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Analyze gradients\n",
    "    gradient_info = {}\n",
    "    total_grad_norm = 0\n",
    "    param_count = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            gradient_info[name] = {\"grad_norm\": grad_norm, \"param_shape\": param.shape, \"param_count\": param.numel()}\n",
    "            total_grad_norm += grad_norm**2\n",
    "            param_count += param.numel()\n",
    "\n",
    "    total_grad_norm = total_grad_norm**0.5\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"total_grad_norm\": total_grad_norm,\n",
    "        \"gradient_info\": gradient_info,\n",
    "        \"trainable_param_count\": param_count,\n",
    "    }\n",
    "\n",
    "\n",
    "# Create final model with comprehensive LoRA integration\n",
    "print(\"Creating production-ready LoRA model\")\n",
    "final_model = MultilayerPerceptron(100, 200, 300, 50).to(device)\n",
    "\n",
    "# Apply LoRA to all layers with different ranks based on layer importance\n",
    "layer_configs = [\n",
    "    {\"layer_idx\": 0, \"rank\": 8, \"alpha\": 16.0},  # Input layer - higher rank\n",
    "    {\"layer_idx\": 2, \"rank\": 4, \"alpha\": 8.0},  # Hidden layer - medium rank\n",
    "    {\"layer_idx\": 4, \"rank\": 8, \"alpha\": 16.0},  # Output layer - higher rank\n",
    "]\n",
    "\n",
    "print(\"Applying layer-specific LoRA configurations:\")\n",
    "for config in layer_configs:\n",
    "    layer = final_model.layers[config[\"layer_idx\"]]\n",
    "    lora_layer = LinearWithLoRA(layer, rank=config[\"rank\"], alpha=config[\"alpha\"]).to(device)\n",
    "    final_model.layers[config[\"layer_idx\"]] = lora_layer\n",
    "    print(f\"  Layer {config['layer_idx']}: rank={config['rank']}, alpha={config['alpha']}\")\n",
    "\n",
    "# Freeze parameters appropriately\n",
    "frozen_count, trainable_count = freeze_linear_layers(final_model, exclude_lora=True)\n",
    "\n",
    "print(\"\\nParameter freezing results:\")\n",
    "print(f\"  Frozen parameters: {frozen_count:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"  Total parameters: {frozen_count + trainable_count:,}\")\n",
    "print(f\"  Training efficiency: {(frozen_count + trainable_count) / trainable_count:.1f}x reduction\")\n",
    "\n",
    "# Detailed parameter analysis\n",
    "print(\"\\nDetailed parameter breakdown:\")\n",
    "for name, param in final_model.named_parameters():\n",
    "    status = \"TRAINABLE\" if param.requires_grad else \"FROZEN\"\n",
    "    print(f\"  {name:<30} {str(param.shape):<20} {param.numel():<8,} {status}\")\n",
    "\n",
    "# Test gradient flow\n",
    "print(\"\\nTesting gradient flow:\")\n",
    "sample_input = torch.randn(4, 100).to(device)\n",
    "sample_target = torch.randint(0, 50, (4,)).to(device)\n",
    "\n",
    "gradient_analysis = analyze_gradient_flow(final_model, sample_input, sample_target)\n",
    "\n",
    "print(\"Gradient flow analysis:\")\n",
    "print(f\"  Loss value: {gradient_analysis['loss']:.6f}\")\n",
    "print(f\"  Total gradient norm: {gradient_analysis['total_grad_norm']:.6f}\")\n",
    "print(f\"  Trainable parameters with gradients: {gradient_analysis['trainable_param_count']:,}\")\n",
    "\n",
    "print(\"\\nLoRA layer gradient details:\")\n",
    "for name, info in gradient_analysis[\"gradient_info\"].items():\n",
    "    if \"lora\" in name.lower() or any(x in name for x in [\"A\", \"B\"]):\n",
    "        print(f\"  {name:<25} grad_norm: {info['grad_norm']:.6f}\")\n",
    "\n",
    "# Verify training readiness\n",
    "trainable_with_grads = len(\n",
    "    [name for name in gradient_analysis[\"gradient_info\"] if any(x in name for x in [\"lora\", \"A\", \"B\"])]\n",
    ")\n",
    "\n",
    "print(\"\\nTraining readiness check:\")\n",
    "print(f\"  LoRA parameters with gradients: {trainable_with_grads}\")\n",
    "print(f\"  Status: {'READY FOR TRAINING' if trainable_with_grads > 0 else 'CHECK CONFIGURATION'}\")\n",
    "\n",
    "print(\"\\nLoRA integration complete - model ready for parameter-efficient fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "### Technical Concepts Learned\n",
    "- **Low-Rank Decomposition**: Understanding ΔW = A × B^T factorization for parameter-efficient weight updates\n",
    "- **LoRA Initialization**: Gaussian initialization for A matrix, zero initialization for B to ensure stable training start\n",
    "- **Parameter Freezing**: Keeping base model weights frozen while only training low-rank adaptation matrices\n",
    "- **Rank and Alpha Selection**: Controlling adaptation capacity (rank) and strength (alpha/rank scaling factor)\n",
    "- **Integration Strategies**: Applying LoRA selectively to different layers (all, output-only, attention layers)\n",
    "\n",
    "### Experiment Further\n",
    "- Apply LoRA to transformer attention layers (Q, K, V, O projections) and compare efficiency\n",
    "- Implement QLoRA combining quantization with LoRA for extreme memory efficiency\n",
    "- Compare different rank values (1, 4, 16, 64) on a text classification task\n",
    "- Try multiple LoRA adapters on the same base model for multi-task learning\n",
    "- Experiment with AdaLoRA for automatic rank allocation during training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
